import os
import time
import re
import io
import pandas as pd

from typing import Dict, Any, Optional, List
from app.models.language.visualization import (
    VisualizationRequest,
    VisualizationType,
    OutputFormat
)
from app.utils.language.generator import call_llm
from app.core.config import settings
from app.utils.logger.setup import setup_logger
from app.prompts.language.visualization import (
    get_table_prompt,
    get_chart_prompt
)
from app.services.language.language_detection.detector import detect_language_with_ai
from app.prompts.language.visualization.visualization_analysis import get_analysis_prompt
import base64
import json
import boto3
from datetime import datetime

logger = setup_logger("visualization_generator")

class VisualizationGenerator:
    """ë¬¸ì„œ ì‹œê°í™” ìƒì„±ê¸°"""
    
    def __init__(self):
        self.output_dir = os.path.join(settings.output_dir, "visualization")
        os.makedirs(self.output_dir, exist_ok=True)

        # NCP S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.s3_client = boto3.client(
            service_name=settings.naver_service_name,
            endpoint_url=settings.naver_endpoint_url,
            aws_access_key_id=settings.ncp_access_key,
            aws_secret_access_key=settings.ncp_secret_key
        )
    
    async def generate_visualization_from_text(self, request: VisualizationRequest) -> Dict[str, Any]:
        """
        ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œë¶€í„° ì‹œê°í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. (í‘œ/CSV í˜•íƒœë¡œ ì§ì ‘ êµ¬ì¡°í™”)

        Args:
            request: ì‹œê°í™” ìš”ì²­ ë°ì´í„°

        Returns:
            Dict: ìƒì„±ëœ ì‹œê°í™” ê²°ê³¼
        """
        try:
            logger.info(f"í…ìŠ¤íŠ¸ ì‹œê°í™” ìƒì„± ì‹œì‘: {request.visualization_type.value}")
            logger.info("ğŸ“ ì¼ë°˜ í…ìŠ¤íŠ¸ ì²˜ë¦¬ â†’ í‘œ í˜•íƒœë¡œ ì§ì ‘ êµ¬ì¡°í™”")

            # í…ìŠ¤íŠ¸ë¥¼ í‘œ/CSV í˜•íƒœë¡œ ì§ì ‘ êµ¬ì¡°í™” (JSON ë‹¨ê³„ ìƒëµ)
            structured_table = await self._structure_content_to_table(request.content, "text", request.visualization_type)
            request.content = structured_table

            return await self._generate_visualization_content(request)

        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {
                "status": "error",
                "error": f"í…ìŠ¤íŠ¸ ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }

    async def generate_visualization(self, request: VisualizationRequest, content_type: str = "file") -> Dict[str, Any]:
        """
        íŒŒì¼ë¡œë¶€í„° ì‹œê°í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            request: ì‹œê°í™” ìš”ì²­ ë°ì´í„°
            content_type: ì½˜í…ì¸  íƒ€ì… ("csv", "excel", "pdf", "image", "text")

        Returns:
            Dict: ìƒì„±ëœ ì‹œê°í™” ê²°ê³¼
        """
        try:
            logger.info(f"íŒŒì¼ ì‹œê°í™” ìƒì„± ì‹œì‘: {request.visualization_type.value}, íƒ€ì…: {content_type}")

            # 1ë‹¨ê³„: í‘œ í˜•ì‹ ë°ì´í„° ê°ì§€ ì‹œë„
            is_table_format = self._detect_table_format(request.content)

            if content_type in ["csv", "excel"] or is_table_format:
                # CSV/Excel ë˜ëŠ” í‘œ í˜•ì‹ PDF/ì´ë¯¸ì§€ - DataFrameìœ¼ë¡œ ì§ì ‘ ì²˜ë¦¬
                logger.info(f"ğŸ“Š í‘œ í˜•ì‹ ë°ì´í„° ê°ì§€ - DataFrameìœ¼ë¡œ ì§ì ‘ ì²˜ë¦¬ (íƒ€ì…: {content_type})")

                validation_result = self._validate_and_extract_data(request.content, request.visualization_type)
                if not validation_result["is_valid"]:
                    return {
                        "status": "error",
                        "error": validation_result["message"]
                    }

                # ì¤‘ìš”: request.contentëŠ” ì›ë³¸ ë°ì´í„°ë¥¼ ìœ ì§€
                # ë‹¤ì¤‘ ì‹œê°í™” ìƒì„±ì„ ìœ„í•´ ì›ë³¸ í‘œ í˜•ì‹ ë°ì´í„° í•„ìš”

            else:
                # í‘œ í˜•ì‹ì´ ì•„ë‹Œ PDF, Image, Text - í‘œ/CSV í˜•íƒœë¡œ ì§ì ‘ êµ¬ì¡°í™” (JSON ë‹¨ê³„ ìƒëµ)
                logger.info(f"ğŸ“Š {content_type.upper()} ì½˜í…ì¸  â†’ í‘œ í˜•íƒœë¡œ ì§ì ‘ êµ¬ì¡°í™”")
                structured_table = await self._structure_content_to_table(request.content, content_type, request.visualization_type)
                request.content = structured_table

            return await self._generate_visualization_content(request)

        except Exception as e:
            logger.error(f"íŒŒì¼ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {
                "status": "error",
                "error": f"íŒŒì¼ ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }

    async def _generate_visualization_content(self, request: VisualizationRequest) -> Dict[str, Any]:
        """
        ê³µí†µ ì‹œê°í™” ì½˜í…ì¸  ìƒì„± ë¡œì§

        Args:
            request: ì‹œê°í™” ìš”ì²­ ë°ì´í„°

        Returns:
            Dict: ìƒì„±ëœ ì‹œê°í™” ê²°ê³¼ (ì—¬ëŸ¬ ì‹œê°í™” í¬í•¨ ê°€ëŠ¥)
        """
        try:
            # ì½˜í…ì¸ ë¥¼ ë¶„ì„í•˜ì—¬ ì—¬ëŸ¬ ì‹œê°í™” ê°€ëŠ¥í•œ ë¶€ë¶„ ì¶”ì¶œ
            visualizable_contents = await self._extract_visualizable_contents(request.content, request.visualization_type)

            if not visualizable_contents:
                logger.warning("ì‹œê°í™” ê°€ëŠ¥í•œ ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return {
                    "status": "error",
                    "error": "ì‹œê°í™” ê°€ëŠ¥í•œ ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                }

            logger.info(f"ì‹œê°í™” ê°€ëŠ¥í•œ ì½˜í…ì¸  {len(visualizable_contents)}ê°œ ë°œê²¬")

            # ê° ì½˜í…ì¸ ì— ëŒ€í•´ ì‹œê°í™” ìƒì„±
            all_ncp_urls = []
            for idx, content_part in enumerate(visualizable_contents):
                logger.info(f"ì‹œê°í™” ìƒì„± ì¤‘ ({idx + 1}/{len(visualizable_contents)})")

                # í”„ë¡¬í”„íŠ¸ ìƒì„±
                prompt = await self._build_visualization_prompt_for_content(content_part, request.visualization_type)

                # LLMìœ¼ë¡œ ì‹œê°í™” ì½˜í…ì¸  ìƒì„±
                try:
                    content = await call_llm(
                        prompt=prompt,
                        model=request.model
                    )
                    # call_llmì€ ì§ì ‘ ë¬¸ìì—´ì„ ë°˜í™˜í•˜ë¯€ë¡œ content.contentë¥¼ ì‚¬ìš©
                    if hasattr(content, 'content'):
                        content = content.content
                    elif isinstance(content, str):
                        content = content
                    else:
                        content = str(content)
                except Exception as e:
                    logger.error(f"LLM ìƒì„± ì‹¤íŒ¨ ({idx + 1}): {str(e)}")
                    continue

                # í›„ì²˜ë¦¬ ë° í¬ë§· ë³€í™˜
                result = await self._process_output(content, request)

                if result.get("ncp_url"):
                    all_ncp_urls.append(result["ncp_url"])

            if not all_ncp_urls:
                return {
                    "status": "error",
                    "error": "ì‹œê°í™” ì´ë¯¸ì§€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤"
                }

            return {
                "model_used": request.model,
                "status": "success",
                "ncp_urls": all_ncp_urls
            }

        except Exception as e:
            error_msg = f"ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"
            logger.error(error_msg)
            return {
                "status": "error",
                "error": error_msg
            }
    
    async def _extract_visualizable_contents(self, content: str, viz_type: VisualizationType) -> list:
        """
        ì½˜í…ì¸ ì—ì„œ ì‹œê°í™” ê°€ëŠ¥í•œ ë¶€ë¶„ë“¤ì„ ëª¨ë‘ ì¶”ì¶œí•©ë‹ˆë‹¤.
        DataFrameìœ¼ë¡œ ë³€í™˜ ê°€ëŠ¥í•œ ê²½ìš° ì—¬ëŸ¬ ê´€ì ì˜ ì‹œê°í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            content: ì›ë³¸ ì½˜í…ì¸ 
            viz_type: ì‹œê°í™” íƒ€ì…

        Returns:
            list: ì‹œê°í™” ê°€ëŠ¥í•œ ì½˜í…ì¸  ëª©ë¡
        """
        try:
            # 1. DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë‹¤ì–‘í•œ ì‹œê°í™” ìƒì„± ì‹œë„ (ìµœìš°ì„ )
            visualizations = await self._extract_multiple_visualizations(content, viz_type)
            if visualizations and len(visualizations) > 1:
                logger.info(f"âœ… DataFrame ê¸°ë°˜ ë‹¤ì¤‘ ì‹œê°í™”: {len(visualizations)}ê°œ ì¶”ì¶œ")
                return visualizations

            # 2. í‘œ í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ì°¾ê¸° (ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” ë“±)
            tables = self._extract_tables_from_content(content)
            if tables:
                logger.info(f"ğŸ“‹ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” ë°œê²¬: {len(tables)}ê°œ")
                return tables

            # 3. ë¶„í• í•  ìˆ˜ ì—†ëŠ” ê²½ìš° ì „ì²´ ì½˜í…ì¸ ë¥¼ í•˜ë‚˜ì˜ ì‹œê°í™”ë¡œ
            logger.info("â„¹ï¸ ë‹¨ì¼ ì½˜í…ì¸ ë¡œ ì‹œê°í™”")
            return [content]

        except Exception as e:
            logger.error(f"âŒ ì‹œê°í™” ê°€ëŠ¥í•œ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return [content]

    async def _extract_multiple_visualizations(self, content: str, viz_type: VisualizationType) -> list:
        """
        ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì—¬ëŸ¬ ê´€ì ì˜ ì‹œê°í™”ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        ëª¨ë“  ë°ì´í„° í˜•ì‹(CSV, Excel, PDF, ì´ë¯¸ì§€, í…ìŠ¤íŠ¸)ì— ëŒ€ì‘í•©ë‹ˆë‹¤.

        Returns:
            list: ì‹œê°í™” ê°€ëŠ¥í•œ ì½˜í…ì¸  ëª©ë¡ (ê° ê´€ì ë³„)
        """
        try:
            # DataFrame ë³€í™˜ ì‹œë„
            df_result = self._extract_dataframe_from_text(content)
            if not df_result.get("success"):
                return []

            df = df_result["dataframe"]

            # ìµœì†Œ 2í–‰, 2ì—´ ì´ìƒ í•„ìš”
            if len(df) < 2 or len(df.columns) < 2:
                return []

            logger.info(f"DataFrame íŒŒì‹± ì„±ê³µ: {df.shape[0]}í–‰ x {df.shape[1]}ì—´")
            logger.info(f"ì›ë³¸ DataFrame dtypes:\n{df.dtypes}")

            # ì»¬ëŸ¼ íƒ€ì… ë¶„ì„ ë° ìë™ ë³€í™˜
            column_analysis = self._analyze_column_types(df)
            numeric_cols = column_analysis['numeric']
            categorical_cols = column_analysis['categorical']
            temporal_cols = column_analysis['temporal']

            # ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ê°ì§€ëœ ì»¬ëŸ¼ì„ numeric dtypeìœ¼ë¡œ ë³€í™˜
            df = self._convert_numeric_columns(df, numeric_cols)

            logger.info(f"ë³€í™˜ í›„ DataFrame dtypes:\n{df.dtypes}")
            logger.info(f"ìˆ˜ì¹˜í˜• ì»¬ëŸ¼: {numeric_cols}")
            logger.info(f"ë²”ì£¼í˜• ì»¬ëŸ¼: {categorical_cols}")
            logger.info(f"ì‹œê³„ì—´ ì»¬ëŸ¼: {temporal_cols}")

            # ì‹œê°í™” ê°€ëŠ¥í•œ ì¡°í•©ì´ ìˆëŠ”ì§€ í™•ì¸
            if not numeric_cols:
                logger.info("ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ ì—†ì–´ ë‹¤ì¤‘ ì‹œê°í™” ë¶ˆê°€")
                return []

            # ì‹œê°í™” ê°€ëŠ¥í•œ ì¡°í•© ìƒì„±
            visualizations = []

            # 1. ì‹œê³„ì—´ + ê° ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ (ì˜ˆ: ë…„ë„ë³„ íŒë§¤ëŸ‰, ë…„ë„ë³„ ìˆ˜ìµ)
            if temporal_cols and numeric_cols:
                for time_col in temporal_cols[:1]:  # ì²« ë²ˆì§¸ ì‹œê°„ ì»¬ëŸ¼ë§Œ ì‚¬ìš©
                    for num_col in numeric_cols[:3]:  # ìµœëŒ€ 3ê°œ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼
                        subset = df[[time_col, num_col]].copy()

                        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ ì‹¤ì œë¡œ numeric dtypeì¸ì§€ í™•ì¸ ë° ë³€í™˜
                        if not pd.api.types.is_numeric_dtype(subset[num_col]):
                            subset[num_col] = pd.to_numeric(subset[num_col], errors='coerce')
                            logger.warning(f"âš ï¸ ì‹œê³„ì—´ ì§‘ê³„ ì „ '{num_col}' ì»¬ëŸ¼ numeric ë³€í™˜")

                        # ì‹œê³„ì—´ ë°ì´í„°ëŠ” í‰ê· ê°’ìœ¼ë¡œ ì§‘ê³„ (ì¶”ì´ë¥¼ ë³´ê¸°ì— ì í•©)
                        aggregated = subset.groupby(time_col)[num_col].mean().reset_index()

                        # ì˜ë¯¸ ìˆëŠ” ì‹œê°í™” í•„í„°ë§: ìµœì†Œ 2ê°œ ì´ìƒì˜ ì‹œì  í•„ìš”
                        if len(aggregated) < 2:
                            logger.info(f"â­ï¸ ì‹œê°í™” ì œì™¸ (ë‹¨ì¼ ì‹œì ): {time_col} vs {num_col} - {len(aggregated)}ê°œ ì‹œì ")
                            continue

                        # ì§‘ê³„ ê²°ê³¼ ê²€ì¦
                        logger.info(f"ğŸ“Š ì‹œê³„ì—´ ì§‘ê³„ ê²°ê³¼ ({time_col} vs {num_col}):")
                        logger.info(f"   {aggregated.to_string()}")

                        # CSVì— ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ì£¼ì„ í˜•íƒœë¡œ)
                        viz_content = f"# AGGREGATION: í‰ê·  (mean) - {time_col}ë³„ {num_col}ì˜ í‰ê· ê°’\n"
                        viz_content += aggregated.to_csv(index=False)
                        visualizations.append(viz_content)
                        logger.info(f"ì‹œê³„ì—´ ì‹œê°í™” ì¶”ê°€ (í‰ê·  ì§‘ê³„): {time_col} vs {num_col}")

            # 2. ë²”ì£¼í˜• + ìˆ˜ì¹˜í˜• (ì˜ˆ: ì œí’ˆë³„ íŒë§¤ëŸ‰, ì§€ì—­ë³„ ìˆ˜ìµ)
            if categorical_cols and numeric_cols:
                for cat_col in categorical_cols[:2]:  # ìµœëŒ€ 2ê°œ ë²”ì£¼í˜• ì»¬ëŸ¼
                    for num_col in numeric_cols[:2]:  # ìµœëŒ€ 2ê°œ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼
                        # ì‹œê³„ì—´ê³¼ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡
                        if cat_col not in temporal_cols:
                            subset = df[[cat_col, num_col]].copy()

                            # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ ì‹¤ì œë¡œ numeric dtypeì¸ì§€ í™•ì¸ ë° ë³€í™˜
                            if not pd.api.types.is_numeric_dtype(subset[num_col]):
                                subset[num_col] = pd.to_numeric(subset[num_col], errors='coerce')
                                logger.warning(f"âš ï¸ ì§‘ê³„ ì „ '{num_col}' ì»¬ëŸ¼ numeric ë³€í™˜")

                            # ë²”ì£¼ë³„ ì§‘ê³„ (NaN ì œì™¸)
                            aggregated = subset.groupby(cat_col)[num_col].sum().reset_index()

                            # ì˜ë¯¸ ìˆëŠ” ì‹œê°í™” í•„í„°ë§: ìµœì†Œ 2ê°œ ì´ìƒì˜ ë²”ì£¼ í•„ìš”
                            if len(aggregated) < 2:
                                logger.info(f"â­ï¸ ì‹œê°í™” ì œì™¸ (ë‹¨ì¼ ë²”ì£¼): {cat_col} vs {num_col} - {len(aggregated)}ê°œ ë²”ì£¼")
                                continue

                            # ì§‘ê³„ ê²°ê³¼ ê²€ì¦
                            logger.info(f"ğŸ“Š ì§‘ê³„ ê²°ê³¼ ({cat_col} vs {num_col}):")
                            logger.info(f"   {aggregated.to_string()}")

                            # CSVì— ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ì£¼ì„ í˜•íƒœë¡œ)
                            viz_content = f"# AGGREGATION: í•©ê³„ (sum) - {cat_col}ë³„ {num_col}ì˜ í•©ê³„\n"
                            viz_content += aggregated.to_csv(index=False)
                            visualizations.append(viz_content)
                            logger.info(f"ë²”ì£¼í˜• ì‹œê°í™” ì¶”ê°€ (í•©ê³„ ì§‘ê³„): {cat_col} vs {num_col}")

            # ì‹œê°í™”ê°€ ìƒì„±ë˜ì§€ ì•Šì€ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ì˜ë¯¸ ì—†ëŠ” ì‹œê°í™” ë°©ì§€)
            if not visualizations:
                logger.info("âš ï¸ ì˜ë¯¸ ìˆëŠ” ì‹œê°í™”ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ë‹¨ì¼ ë²”ì£¼/ì‹œì ë§Œ ì¡´ì¬)")
                return []

            # ìµœëŒ€ 10ê°œë¡œ ì œí•œ
            # if len(visualizations) > 10:
            #     visualizations = visualizations[:10]
            #     logger.info(f"ì‹œê°í™” ê°œìˆ˜ë¥¼ 10ê°œë¡œ ì œí•œ")

            return visualizations

        except Exception as e:
            logger.warning(f"ë‹¤ì¤‘ ì‹œê°í™” ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return []

    def _extract_tables_from_content(self, content: str) -> list:
        """
        ì½˜í…ì¸ ì—ì„œ í‘œ í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            content: ì›ë³¸ ì½˜í…ì¸ 

        Returns:
            list: ì¶”ì¶œëœ í‘œ ëª©ë¡
        """
        tables = []
        lines = content.split('\n')
        current_table = []

        for line in lines:
            # ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” ë¼ì¸ ê°ì§€
            if '|' in line.strip() and line.strip().count('|') >= 2:
                current_table.append(line)
            else:
                # í…Œì´ë¸”ì´ ëë‚œ ê²½ìš°
                if current_table and len(current_table) >= 3:  # ìµœì†Œ í—¤ë” + êµ¬ë¶„ì„  + ë°ì´í„° 1í–‰
                    tables.append('\n'.join(current_table))
                current_table = []

        # ë§ˆì§€ë§‰ í…Œì´ë¸” ì²˜ë¦¬
        if current_table and len(current_table) >= 3:
            tables.append('\n'.join(current_table))

        return tables

    async def _build_visualization_prompt_for_content(self, content: str, viz_type: VisualizationType) -> str:
        """
        íŠ¹ì • ì½˜í…ì¸ ì— ëŒ€í•œ ì‹œê°í™” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            content: ì‹œê°í™”í•  ì½˜í…ì¸ 
            viz_type: ì‹œê°í™” íƒ€ì…

        Returns:
            str: ìƒì„±ëœ í”„ë¡¬í”„íŠ¸
        """
        # AI ê¸°ë°˜ ì–¸ì–´ ê°ì§€
        try:
            detection_result = await detect_language_with_ai(content)
            lang_code = detection_result.get("primary_language")
            confidence = detection_result.get("confidence", 0.0)
            logger.info(f"ğŸŒ AI ì–¸ì–´ ê°ì§€: {lang_code}, ì‹ ë¢°ë„: {confidence:.2f}")
        except Exception as e:
            logger.warning(f"AI ì–¸ì–´ ê°ì§€ ì‹¤íŒ¨, ê¸°ë³¸ê°’(en) ì‚¬ìš©: {str(e)}")
            lang_code = 'en'

        # ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸ ì„ íƒ
        if viz_type == VisualizationType.TABLE:
            base_prompt = get_table_prompt(lang_code)
        elif viz_type == VisualizationType.CHART:
            base_prompt = get_chart_prompt(lang_code)

        # ì¶œë ¥ í˜•ì‹ ì§€ì • (ê¸°ë³¸ê°’: IMAGE_PNG)
        output_format = OutputFormat.IMAGE_PNG
        format_instruction = self._get_format_instruction(output_format)
        base_prompt += f"\n\n{format_instruction}"

        # ì§‘ê³„ ë©”íƒ€ë°ì´í„° í™•ì¸
        aggregation_instruction = ""
        if content.startswith("# AGGREGATION:"):
            lines = content.split('\n')
            agg_line = lines[0]
            if "í‰ê· " in agg_line or "mean" in agg_line.lower():
                aggregation_instruction = "\n\nì¤‘ìš”: ì´ ë°ì´í„°ëŠ” í‰ê· ê°’ìœ¼ë¡œ ì§‘ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ì°¨íŠ¸ ì œëª©ì— ë°˜ë“œì‹œ 'í‰ê· 'ì„ í¬í•¨í•˜ì„¸ìš”. (ì˜ˆ: 'ì—°ë„ë³„ í‰ê·  íŒë§¤ëŸ‰ ì¶”ì´')"
            elif "í•©ê³„" in agg_line or "sum" in agg_line.lower():
                aggregation_instruction = "\n\nì¤‘ìš”: ì´ ë°ì´í„°ëŠ” í•©ê³„ë¡œ ì§‘ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ì°¨íŠ¸ ì œëª©ì— ë°˜ë“œì‹œ 'í•©ê³„' ë˜ëŠ” 'ì´'ì„ í¬í•¨í•˜ì„¸ìš”. (ì˜ˆ: 'ì œí’ˆë³„ ì´ íŒë§¤ëŸ‰')"

        # ì½˜í…ì¸  ì¶”ê°€
        full_prompt = f"{base_prompt}{aggregation_instruction}\n\në¶„ì„í•  ë¬¸ì„œ ë‚´ìš©:\n{content}"

        return full_prompt

    async def _build_visualization_prompt(self, request: VisualizationRequest) -> str:
        """ì‹œê°í™” í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤. (í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)"""
        return await self._build_visualization_prompt_for_content(request.content, request.visualization_type)
    
    def _get_format_instruction(self, output_format: OutputFormat) -> str:
        """ì¶œë ¥ í˜•ì‹ ì§€ì‹œì‚¬í•­"""
        instructions = {
            OutputFormat.IMAGE_PNG: "ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ìƒì„±í•œ í›„ PNG ì´ë¯¸ì§€ë¡œ ë³€í™˜í•  ì˜ˆì •ì…ë‹ˆë‹¤."
        }
        return instructions.get(output_format, instructions[OutputFormat.IMAGE_PNG])
    
    async def _process_output(self, content: str, request: VisualizationRequest) -> Dict[str, Any]:
        """ì¶œë ¥ ì½˜í…ì¸ ë¥¼ í›„ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        
        result = {
            "content": content,
            "image_base64": None,
            "image_path": None
        }
        
        # ì´ë¯¸ì§€ ë³€í™˜ì´ í•„ìš”í•œ ê²½ìš° (ê¸°ë³¸ì ìœ¼ë¡œ í•­ìƒ ì´ë¯¸ì§€ë¡œ ë³€í™˜)
        output_format = OutputFormat.IMAGE_PNG
        if output_format in [OutputFormat.IMAGE_PNG]:
            try:
                image_result = await self._convert_to_image(content, request)
                result.update(image_result)
            except Exception as e:
                logger.warning(f"ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨ {str(e)}")
        
        return result
    
    async def _convert_to_image(self, content: str, request: VisualizationRequest) -> Dict[str, Any]:
        """ì½˜í…ì¸ ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ê³  NCPì— ì—…ë¡œë“œ"""

        # visualization í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
        os.makedirs(self.output_dir, exist_ok=True)

        # ì„ì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±
        timestamp = int(time.time())
        filename = f"{request.visualization_type.value}_{timestamp}.png"
        image_path = os.path.join(self.output_dir, filename)

        try:
            # ì‹œê°í™” íƒ€ì…ë³„ ë Œë”ë§ ì‹¤í–‰ (íŒŒì¼ë¡œ)
            if request.visualization_type in [VisualizationType.TABLE, VisualizationType.CHART]:
                # ìŠ¤íƒ€ì¼ ì˜µì…˜ ì¤€ë¹„
                style_options = {
                    "figsize": (12, 8),
                    "dpi": 300,
                    "font_size": 12,
                    "renderer": "matplotlib"
                }

                # ìš”ì²­ ì˜µì…˜ ë³‘í•©
                if request.image_options:
                    style_options.update(request.image_options)

                # ì‹œê°í™” íƒ€ì…ë³„ ë Œë”ë§ ì‹¤í–‰
                if request.visualization_type == VisualizationType.TABLE:
                    from app.services.language.visualization.table_renderer import TableRenderer
                    renderer = TableRenderer()

                    # í‘œ ë°ì´í„° ì²˜ë¦¬ (ì œëª© í¬í•¨)
                    parsed_result = renderer.parse_csv_from_llm_response(content)
                    csv_data = parsed_result["csv_data"]
                    table_title = parsed_result.get("title")

                    # ì œëª©ì´ ìˆìœ¼ë©´ style_optionsì— ì¶”ê°€
                    if table_title:
                        style_options["title"] = table_title
                        logger.info(f"í‘œ ì œëª© ì¶”ê°€: '{table_title}'")

                    # CSV ìœ íš¨ì„± ê²€ì¦
                    renderer.validate_csv_data(csv_data)

                    # í‘œ ì´ë¯¸ì§€ íŒŒì¼ ìƒì„±
                    await renderer.render_table_auto(csv_data, image_path, style_options)
                    logger.info(f"í‘œ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: {image_path}")

                elif request.visualization_type == VisualizationType.CHART:
                    from app.services.language.visualization.chart_renderer import ChartRenderer
                    renderer = ChartRenderer()

                    # ì°¨íŠ¸ ë°ì´í„° íŒŒì‹± ë° ê²€ì¦
                    renderer.validate_chart_data(content)

                    # ì°¨íŠ¸ ì´ë¯¸ì§€ íŒŒì¼ ìƒì„±
                    await renderer.render_chart_auto(content, image_path, style_options)
                    logger.info(f"ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ: {image_path}")

            # íŒŒì¼ì´ ì‹¤ì œë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if not os.path.exists(image_path):
                raise ValueError(f"ì´ë¯¸ì§€ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {image_path}")

            # ë¡œì»¬ íŒŒì¼ì„ ì½ì–´ì„œ NCPì— ì—…ë¡œë“œ
            ncp_url = await self.upload_file_to_ncp(image_path, request.visualization_type.value)
            logger.info(f"ì‹œê°í™” ì´ë¯¸ì§€ NCP ì—…ë¡œë“œ ì™„ë£Œ: {ncp_url}")

            return {
                "content": f"![{request.visualization_type.value}]({ncp_url})",
                "image_base64": None,  # base64 ëŒ€ì‹  URL ì‚¬ìš©
                "image_path": image_path,
                "ncp_url": ncp_url
            }

        except Exception as e:
            error_types = {"table": "í‘œ", "chart": "ì°¨íŠ¸"}
            error_type = error_types.get(request.visualization_type.value, "ì‹œê°í™”")
            logger.error(f"{error_type} ë Œë”ë§ ì‹¤íŒ¨: {str(e)}")
            raise e

        finally:
            # NCP ì—…ë¡œë“œ í›„ ë¡œì»¬ íŒŒì¼ ìë™ ì‚­ì œ
            if os.path.exists(image_path):
                try:
                    os.remove(image_path)
                    logger.info(f"ë¡œì»¬ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {image_path}")
                except Exception as delete_error:
                    logger.warning(f"ë¡œì»¬ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {delete_error}")
    
    def _validate_and_extract_data(self, content: str, viz_type: VisualizationType) -> Dict[str, Any]:
        """ì½˜í…ì¸  ê²€ì¦ ë° ì‹œê°í™” ê°€ëŠ¥í•œ ë°ì´í„° ì¶”ì¶œ"""
        
        content_lower = content.lower()
        
        # í‘œ/ì°¨íŠ¸ì— ì í•©í•œ ë°ì´í„° íŒ¨í„´ ê²€ì‚¬
        if viz_type in [VisualizationType.TABLE, VisualizationType.CHART]:
            # ë¨¼ì € ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” í™•ì¸
            pipe_lines = [line for line in content.split('\n') if '|' in line.strip()]
            
            if len(pipe_lines) >= 3:  # ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”ë¡œ ê°„ì£¼
                logger.info(f"ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” ê°ì§€: {len(pipe_lines)}ê°œ ë¼ì¸")
                table_data = '\n'.join(pipe_lines)

                # DataFrame ë³€í™˜ ì‹œë„
                df_result = self._extract_dataframe_from_text(table_data)
                if df_result["success"]:
                    df = df_result["dataframe"]
                    logger.info(f"âœ… DataFrame êµ¬ì¡°í™” ì„±ê³µ: {df.shape[0]}í–‰ x {df.shape[1]}ì—´")
                    logger.info(f"ğŸ“Š ì»¬ëŸ¼ ëª©ë¡: {list(df.columns)}")
                    logger.info(f"ğŸ” DataFrame ìƒì„¸ ë‚´ìš©:\n{df.to_string()}")

                    # ë³€ìˆ˜ íƒ€ì… ë¶„ì„ ë¡œê·¸
                    column_analysis = self._analyze_column_types(df)
                    logger.info(f"ğŸ·ï¸ ë³€ìˆ˜ íƒ€ì… ë¶„ì„:")
                    logger.info(f"  - ìˆ˜ì¹˜í˜• ë³€ìˆ˜ (Yì¶•ìš©): {column_analysis['numeric']}")
                    logger.info(f"  - ë²”ì£¼í˜• ë³€ìˆ˜ (Xì¶•/ê·¸ë£¹ìš©): {column_analysis['categorical']}")
                    logger.info(f"  - ì‹œê³„ì—´ ë³€ìˆ˜: {column_analysis['temporal']}")
                    logger.info(f"  - í˜¼í•©í˜• ë³€ìˆ˜: {column_analysis['mixed']}")

                    # LLMì„ ìœ„í•œ í¬ë§·íŒ…ëœ ë°ì´í„° ìƒì„±
                    formatted_data = self._format_dataframe_for_llm(df, viz_type)
                    return {
                        "is_valid": True,
                        "message": "DataFrame êµ¬ì¡°í™” ì„±ê³µ",
                        "extracted_data": formatted_data
                    }
                else:
                    logger.warning(f"DataFrame ë³€í™˜ ì‹¤íŒ¨, ì›ë³¸ ë°ì´í„° ì‚¬ìš©: {df_result['error']}")

                return {
                    "is_valid": True,
                    "message": "ê²€ì¦ í†µê³¼",
                    "extracted_data": table_data
                }
            
            # ì›ë³¸ ë°ì´í„°ë¡œ DataFrame ë³€í™˜ ë¨¼ì € ì‹œë„
            df_result = self._extract_dataframe_from_text(content)

            # DataFrame ë³€í™˜ì´ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë§Œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ ì‹œë„
            if not df_result["success"]:
                logger.info("ì›ë³¸ ë°ì´í„° DataFrame ë³€í™˜ ì‹¤íŒ¨, êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ ì‹œë„")
                extracted_data = self._extract_structured_data(content)

                if not extracted_data:
                    return {
                        "is_valid": False,
                        "message": "í•´ë‹¹ ì»¨í…ì¸ ëŠ” ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    }

                # ì¶”ì¶œëœ ë°ì´í„°ë¡œ DataFrame ë³€í™˜ ì¬ì‹œë„
                df_result = self._extract_dataframe_from_text(extracted_data)
            if df_result["success"]:
                df = df_result["dataframe"]
                logger.info(f"âœ… DataFrame êµ¬ì¡°í™” ì„±ê³µ (êµ¬ì¡°í™”ëœ ë°ì´í„°): {df.shape[0]}í–‰ x {df.shape[1]}ì—´")
                logger.info(f"ğŸ“Š ì»¬ëŸ¼ ëª©ë¡: {list(df.columns)}")
                logger.info(f"ğŸ“‹ ì»¬ëŸ¼ë³„ ë°ì´í„° íƒ€ì…:")

                logger.info(f"ğŸ” DataFrame ìƒì„¸ ë‚´ìš©:\n{df.to_string()}")

                # ë³€ìˆ˜ íƒ€ì… ë¶„ì„ ë¡œê·¸
                column_analysis = self._analyze_column_types(df)
                logger.info(f"ğŸ·ï¸ ë³€ìˆ˜ íƒ€ì… ë¶„ì„:")
                logger.info(f"  - ìˆ˜ì¹˜í˜• ë³€ìˆ˜ (Yì¶•ìš©): {column_analysis['numeric']}")
                logger.info(f"  - ë²”ì£¼í˜• ë³€ìˆ˜ (Xì¶•/ê·¸ë£¹ìš©): {column_analysis['categorical']}")
                logger.info(f"  - ì‹œê³„ì—´ ë³€ìˆ˜: {column_analysis['temporal']}")
                logger.info(f"  - í˜¼í•©í˜• ë³€ìˆ˜: {column_analysis['mixed']}")

                # LLMì„ ìœ„í•œ í¬ë§·íŒ…ëœ ë°ì´í„° ìƒì„±
                formatted_data = self._format_dataframe_for_llm(df, viz_type)
                return {
                    "is_valid": True,
                    "message": "DataFrame êµ¬ì¡°í™” ì„±ê³µ",
                    "extracted_data": formatted_data
                }
            else:
                logger.info(f"DataFrame ë³€í™˜ ì‹¤íŒ¨, ì›ë³¸ ì¶”ì¶œ ë°ì´í„° ì‚¬ìš©: {df_result['error']}")

            return {
                "is_valid": True,
                "message": "ê²€ì¦ í†µê³¼",
                "extracted_data": extracted_data
            }
        
        return {"is_valid": True, "message": "ê²€ì¦ í†µê³¼"}
    
    def _extract_structured_data(self, content: str) -> str:
        """í˜¼í•© ë¬¸ì„œì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„°ë§Œ ì¶”ì¶œ"""
        
        lines = content.split('\n')
        extracted_lines = []
        
        # êµ¬ì¡°í™”ëœ ë°ì´í„° íŒ¨í„´ë“¤
        data_patterns = [
            r'^\s*-\s*\d+ë…„.*:\s*\d+',                    # - 2020ë…„: 3ë§Œ ëŒ€
            r'^\s*\d+[ë…„ì›”ì¼].*:\s*\d+',                  # 2020ë…„: 3ë§Œ ëŒ€  
            r'^\s*[ê°€-í£A-Za-z0-9\s]+:\s*\d+[ê°€-í£\s]*', # í•­ëª©: ìˆ˜ì¹˜
            r'^\s*\|\s*[^|]+\s*\|\s*[^|]+\s*\|',          # | í•­ëª© | ê°’ |
            r'^\s*\|\s*[^|]*\d+[^|]*\s*\|',               # | ìˆ«ìê°€ í¬í•¨ëœ ì…€ |
            r'^\s*\|\s*[-\s]*\|\s*[-\s]*\|',              # | ----- | ----- | (êµ¬ë¶„ì„ )
            r'^\s*[ê°€-í£A-Za-z0-9\s]+\s+\d+[ê°€-í£\s]*',  # í•­ëª© ìˆ˜ì¹˜
        ]
        
        # ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” í—¤ë” íŒ¨í„´ (í…Œì´ë¸”ì˜ ë§¥ë½ ì œê³µ)
        table_header_patterns = [
            r'^\s*\|\s*êµ¬ë¶„\s*\|',     # | êµ¬ë¶„ | ... |
            r'^\s*\|\s*í•­ëª©\s*\|',     # | í•­ëª© | ... |
            r'^\s*\|\s*[A-Za-z]+\s*\|', # | Feb | Mar | ... |
        ]
        
        # ì œëª©ì´ë‚˜ í—¤ë” íŒ¨í„´ (ë§¥ë½ ì œê³µìš©)
        title_patterns = [
            r'^[^:]*\d+~\d+ë…„.*',  # 2020~2024ë…„ êµ­ë‚´ ì „ê¸°ì°¨...
            r'^.*ì¶”ì„¸$',           # ...ì¶”ì„¸
            r'^.*í˜„í™©$',           # ...í˜„í™©
            r'^.*ë°ì´í„°$',         # ...ë°ì´í„°
        ]
        
        # ì œëª© ì¶”ì¶œ
        title_found = False
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # ì œëª© íŒ¨í„´ í™•ì¸
            if any(re.search(pattern, line) for pattern in title_patterns) and not title_found:
                extracted_lines.append(line)
                title_found = True
                continue
            
            # ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” í—¤ë” í™•ì¸
            if any(re.search(pattern, line) for pattern in table_header_patterns):
                extracted_lines.append(line)
                continue
            
            # ë°ì´í„° íŒ¨í„´ í™•ì¸
            if any(re.search(pattern, line) for pattern in data_patterns):
                extracted_lines.append(line)
                continue
        
        # ì¶”ì¶œëœ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        if not extracted_lines:
            return ""
        
        extracted_content = '\n'.join(extracted_lines)
        
        # ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”ì¸ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
        table_lines = [line for line in lines if line.strip().startswith('|') and line.strip().endswith('|')]
        
        logger.info(f"ì „ì²´ ë¼ì¸ ìˆ˜: {len(lines)}")
        logger.info(f"í…Œì´ë¸” ë¼ì¸ ê°ì§€: {len(table_lines)}ê°œ")
        
        if len(table_lines) >= 2:  # í—¤ë” + ìµœì†Œ 1ê°œ ë°ì´í„° í–‰ (êµ¬ë¶„ì„  ì—†ì–´ë„ í—ˆìš©)
            logger.info(f"ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” ê°ì§€: {len(table_lines)}ê°œ ë¼ì¸")
            return '\n'.join(table_lines)
        
        # ìµœì†Œ ë°ì´í„° ìš”êµ¬ì‚¬í•­ í™•ì¸
        data_lines = [line for line in extracted_lines if any(re.search(pattern, line) for pattern in data_patterns)]
        
        if len(data_lines) < 2:  # ìµœì†Œ 2ê°œ ë°ì´í„° í¬ì¸íŠ¸ í•„ìš”
            return ""
        
        logger.info(f"êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {len(data_lines)}ê°œ ë°ì´í„° í¬ì¸íŠ¸")
        return extracted_content

    def _clean_extracted_text(self, text: str) -> str:
        """
        ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ë©”íƒ€ë°ì´í„° ì œê±°
        - [í˜ì´ì§€ N] íƒœê·¸ ì œê±°
        - ì œëª© ë¼ì¸ ì œê±° (í‘œ í˜•ì‹ ì•ì˜ ì„¤ëª…)
        - ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        """
        lines = text.strip().split('\n')
        cleaned_lines = []

        for line in lines:
            line_stripped = line.strip()

            # ë¹ˆ ë¼ì¸ ì œê±°
            if not line_stripped:
                continue

            # [í˜ì´ì§€ N] íƒœê·¸ ì œê±°
            if re.match(r'^\[í˜ì´ì§€\s+\d+\]$', line_stripped):
                logger.info(f"ğŸ§¹ í˜ì´ì§€ íƒœê·¸ ì œê±°: {line_stripped}")
                continue

            # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ êµ¬ë¶„ì ì œê±°
            if line_stripped in ['```', '```csv', '```tsv', '```table']:
                logger.info(f"ğŸ§¹ ì½”ë“œ ë¸”ë¡ íƒœê·¸ ì œê±°: {line_stripped}")
                continue

            # ì œëª©ìœ¼ë¡œ ë³´ì´ëŠ” ë¼ì¸ ì œê±° (êµ¬ë¶„ìê°€ ì—†ê³  ì§§ì€ ë¼ì¸)
            if (
                len(line_stripped) < 30 and  # ì§§ì€ ë¼ì¸
                ',' not in line_stripped and  # ì½¤ë§ˆ ì—†ìŒ
                '\t' not in line_stripped and  # íƒ­ ì—†ìŒ
                '  ' not in line_stripped and  # ì—°ì† ê³µë°± ì—†ìŒ
                '|' not in line_stripped and  # íŒŒì´í”„ ì—†ìŒ
                not any(char.isdigit() for char in line_stripped)  # ìˆ«ì ì—†ìŒ (ë°ì´í„° ì•„ë‹˜)
            ):
                # ì œëª©ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
                logger.info(f"ğŸ§¹ ì œëª© ë¼ì¸ ì œê±°: {line_stripped}")
                continue

            cleaned_lines.append(line)

        cleaned_text = '\n'.join(cleaned_lines)
        logger.info(f"ğŸ§¹ í…ìŠ¤íŠ¸ ì •ë¦¬ ì™„ë£Œ: {len(lines)}ì¤„ â†’ {len(cleaned_lines)}ì¤„")

        return cleaned_text

    def _extract_dataframe_from_text(self, text: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ì—ì„œ DataFrame ì¶”ì¶œ ì‹œë„"""
        try:
            # BOM ì œê±°
            text = text.replace('\ufeff', '').strip()

            if not text:
                return {"success": False, "error": "ë¹ˆ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤"}

            # í˜ì´ì§€ íƒœê·¸ ë° ì œëª© ë¼ì¸ ì œê±° (LLM ì¶”ì¶œ ê²°ê³¼ ì •ë¦¬)
            text = self._clean_extracted_text(text)

            # 1. CSV í˜•ì‹ ê°ì§€ ë° ì²˜ë¦¬ (ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë°ì´í„° ìš°ì„ )
            csv_result = self._try_parse_csv(text)
            if csv_result["success"]:
                return csv_result

            # 2. íƒ­ êµ¬ë¶„ ë°ì´í„° ì²˜ë¦¬
            if '\t' in text and text.count('\t') > 5:
                try:
                    logger.info("ğŸ” íƒ­ êµ¬ë¶„ ë°ì´í„° ì‹œë„")
                    df = pd.read_csv(io.StringIO(text), sep='\t', header=0)
                    if df.shape[1] > 1:  # ìµœì†Œ 2ê°œ ì´ìƒì˜ ì»¬ëŸ¼
                        logger.info(f"âœ… íƒ­ êµ¬ë¶„ íŒŒì‹± ì„±ê³µ: {df.shape}")
                        return {"success": True, "dataframe": df, "format": "tsv"}
                except Exception as tsv_error:
                    logger.warning(f"âš ï¸ íƒ­ êµ¬ë¶„ íŒŒì‹± ì‹¤íŒ¨: {str(tsv_error)}")

            # 3. ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” ì²˜ë¦¬
            if '|' in text and text.count('|') > 3:
                try:
                    logger.info("ğŸ” ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” ì‹œë„")
                    df = self._parse_markdown_table(text)
                    if df.shape[1] > 1:  # ìµœì†Œ 2ê°œ ì´ìƒì˜ ì»¬ëŸ¼
                        logger.info(f"âœ… ë§ˆí¬ë‹¤ìš´ íŒŒì‹± ì„±ê³µ: {df.shape}")
                        return {"success": True, "dataframe": df, "format": "markdown"}
                except Exception as md_error:
                    logger.warning(f"âš ï¸ ë§ˆí¬ë‹¤ìš´ íŒŒì‹± ì‹¤íŒ¨: {str(md_error)}")

            # 4. ê³µë°± êµ¬ë¶„ ë°ì´í„° ì²˜ë¦¬
            if self._is_whitespace_separated(text):
                try:
                    logger.info("ğŸ” ê³µë°± êµ¬ë¶„ ë°ì´í„° íŒŒì‹± ì‹œë„")
                    df = self._parse_whitespace_separated(text)
                    if df.shape[1] > 1:  # ìµœì†Œ 2ê°œ ì´ìƒì˜ ì»¬ëŸ¼
                        logger.info(f"âœ… ê³µë°± êµ¬ë¶„ íŒŒì‹± ì„±ê³µ: {df.shape}")
                        return {"success": True, "dataframe": df, "format": "whitespace"}
                except Exception as ws_error:
                    logger.warning(f"âš ï¸ ê³µë°± êµ¬ë¶„ íŒŒì‹± ì‹¤íŒ¨: {str(ws_error)}")

            logger.warning("âš ï¸ ëª¨ë“  íŒŒì‹± ë°©ë²• ì‹¤íŒ¨")
            return {"success": False, "error": "êµ¬ì¡°í™” ê°€ëŠ¥í•œ ë°ì´í„° í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

        except Exception as e:
            logger.warning(f"âŒ DataFrame ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return {"success": False, "error": str(e)}

    def _try_parse_csv(self, text: str) -> Dict[str, Any]:
        """CSV í˜•ì‹ íŒŒì‹± ì‹œë„"""
        try:
            lines = text.strip().split('\n')

            if len(lines) < 2:
                return {"success": False, "error": "ë¼ì¸ ìˆ˜ ë¶€ì¡±"}

            # ì²« ë²ˆì§¸ ë¼ì¸ì—ì„œ ì½¤ë§ˆ ê°œìˆ˜ í™•ì¸
            first_line = lines[0].strip()
            comma_count = first_line.count(',')

            # ì½¤ë§ˆê°€ ì—†ìœ¼ë©´ CSVê°€ ì•„ë‹˜
            if comma_count == 0:
                return {"success": False, "error": "ì½¤ë§ˆê°€ ì—†ìŒ"}

            # pandasë¡œ íŒŒì‹± ì‹œë„
            df = pd.read_csv(io.StringIO(text), header=0, encoding='utf-8-sig')

            # ìµœì†Œ 2ê°œ ì´ìƒì˜ ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•¨
            if df.shape[1] < 2:
                logger.warning(f"âš ï¸ ì»¬ëŸ¼ ìˆ˜ ë¶€ì¡±: {df.shape[1]}ê°œ")
                return {"success": False, "error": f"ì»¬ëŸ¼ ìˆ˜ ë¶€ì¡±: {df.shape[1]}ê°œ"}

            # ì²« ë²ˆì§¸ ì»¬ëŸ¼ëª… ê²€ì¦ (ì´ìƒí•œ ì»¬ëŸ¼ëª… ì²´í¬)
            first_col = str(df.columns[0])
            if len(first_col) > 100 or '|' in first_col:
                logger.warning(f"âš ï¸ ë¹„ì •ìƒì ì¸ ì»¬ëŸ¼ëª…: {first_col[:50]}...")
                return {"success": False, "error": "ë¹„ì •ìƒì ì¸ ì»¬ëŸ¼ëª…"}

            return {"success": True, "dataframe": df, "format": "csv"}

        except Exception as e:
            logger.warning(f"âš ï¸ CSV íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            return {"success": False, "error": str(e)}

    def _is_whitespace_separated(self, text: str) -> bool:
        """ê³µë°± êµ¬ë¶„ ë°ì´í„°ì¸ì§€ í™•ì¸"""
        lines = text.strip().split('\n')
        logger.info(f"ğŸ” ê³µë°± êµ¬ë¶„ í˜•ì‹ í™•ì¸ - ì „ì²´ ë¼ì¸ ìˆ˜: {len(lines)}")

        if len(lines) < 2:
            logger.info("âŒ ê³µë°± êµ¬ë¶„ í™•ì¸ ì‹¤íŒ¨: ë¼ì¸ ìˆ˜ ë¶€ì¡±")
            return False

        # ì—°ì†ëœ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ ì—´ì´ ìˆëŠ”ì§€ í™•ì¸
        for i, line in enumerate(lines[:3]):
            whitespace_cols = re.split(r'\s{2,}', line.strip())
            logger.info(f"ğŸ” ë¼ì¸ {i+1}: '{line}' â†’ ê³µë°± ë¶„í•  ê²°ê³¼: {whitespace_cols} (ì»¬ëŸ¼ ìˆ˜: {len(whitespace_cols)})")
            if len(whitespace_cols) >= 2:
                logger.info("âœ… ê³µë°± êµ¬ë¶„ í˜•ì‹ í™•ì¸ë¨")
                return True

        logger.info("âŒ ê³µë°± êµ¬ë¶„ í˜•ì‹ ì•„ë‹˜")
        return False

    def _parse_whitespace_separated(self, text: str) -> pd.DataFrame:
        """
        ê³µë°± êµ¬ë¶„ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
        PyMuPDF ì¶”ì¶œí•œ í‘œ í˜•ì‹ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

        ë©€í‹°ë¼ì¸ í—¤ë”(ì˜ˆ: VQAv2 í—¤ë” ì•„ë˜ val, test-dev ì„œë¸Œí—¤ë”) ì§€ì›
        í‘œ êµ¬ì¡°ë§Œ ì¶”ì¶œ (ìº¡ì…˜, ì œëª© ë“± ìë™ í•„í„°ë§)

        Args:
            text: ê³µë°± êµ¬ë¶„ ë°ì´í„° í…ìŠ¤íŠ¸

        Returns:
            pd.DataFrame: íŒŒì‹±ëœ DataFrame
        """
        lines = text.strip().split('\n')

        if len(lines) < 2:
            raise ValueError("ìµœì†Œ 2ì¤„ ì´ìƒì˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤ (í—¤ë” + ë°ì´í„°)")

        # ë¹ˆ ë¼ì¸ ì œê±°
        lines = [line for line in lines if line.strip()]

        # í‘œ êµ¬ì¡° ì˜ì—­ ê°ì§€ ë° ì¶”ì¶œ
        table_lines = self._extract_table_structure(lines)

        if len(table_lines) < 2:
            raise ValueError("ìœ íš¨í•œ í‘œ êµ¬ì¡°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        logger.info(f"ğŸ“‹ í‘œ êµ¬ì¡° ì¶”ì¶œ: {len(table_lines)}ê°œ ë¼ì¸")

        # ì²« ë²ˆì§¸ ë¼ì¸ì„ í—¤ë”ë¡œ ì‚¬ìš©
        header_line = table_lines[0].strip()
        headers = re.split(r'\s{2,}', header_line)

        logger.info(f"ğŸ“‹ í—¤ë”: {headers} (ì»¬ëŸ¼ ìˆ˜: {len(headers)})")

        # ë°ì´í„° ë¼ì¸ íŒŒì‹± (2ë²ˆì§¸ ë¼ì¸ë¶€í„°)
        # LLMì´ ì •í™•íˆ ì¶”ì¶œí•œ ë°ì´í„°ë¥¼ ì‹ ë¢°í•˜ì—¬ ëª¨ë“  ë¼ì¸ì„ ë°ì´í„°ë¡œ ì²˜ë¦¬
        data = []
        for i, line in enumerate(table_lines[1:], start=2):
            row = re.split(r'\s{2,}', line.strip())

            # ì»¬ëŸ¼ ìˆ˜ê°€ í—¤ë”ì™€ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ ì¡°ì •
            if len(row) < len(headers):
                # ë¶€ì¡±í•œ ê²½ìš° ë¹ˆ ê°’ìœ¼ë¡œ ì±„ì›€
                row.extend([''] * (len(headers) - len(row)))
            elif len(row) > len(headers):
                # ì´ˆê³¼í•˜ëŠ” ê²½ìš° ì˜ë¼ëƒ„
                logger.warning(f"âš ï¸ ë¼ì¸ {i}: ì»¬ëŸ¼ ìˆ˜ ì´ˆê³¼ ({len(row)} > {len(headers)}), ì˜ë¼ëƒ„")
                logger.debug(f"   ì›ë³¸: {line.strip()}")
                logger.debug(f"   íŒŒì‹±: {row}")
                row = row[:len(headers)]

            data.append(row)

        # DataFrame ìƒì„±
        df = pd.DataFrame(data, columns=headers)

        logger.info(f"âœ… ê³µë°± êµ¬ë¶„ DataFrame ìƒì„±: {df.shape[0]}í–‰ x {df.shape[1]}ì—´")
        logger.info(f"ğŸ“Š ì»¬ëŸ¼ ëª©ë¡: {list(df.columns)}")
        logger.info(f"ğŸ“Š ì²« 3í–‰:\n{df.head(3).to_string()}")

        return df

    def _extract_table_structure(self, lines: list) -> list:
        """
        ë¼ì¸ë“¤ì—ì„œ í‘œ êµ¬ì¡° ì˜ì—­ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.

        í‘œì˜ íŠ¹ì§•:
        - ì—°ì†ëœ ë¼ì¸ë“¤ì´ ì¼ê´€ëœ ì»¬ëŸ¼ ìˆ˜ (2ê°œ ì´ìƒ)ë¥¼ ê°€ì§
        - ìµœì†Œ 3ì¤„ ì´ìƒ ì—°ì†

        Args:
            lines: ì „ì²´ í…ìŠ¤íŠ¸ ë¼ì¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            list: í‘œ êµ¬ì¡° ë¼ì¸ë“¤ë§Œ í¬í•¨ëœ ë¦¬ìŠ¤íŠ¸
        """
        # ê° ë¼ì¸ì˜ ì»¬ëŸ¼ ìˆ˜ ê³„ì‚°
        line_column_counts = []
        for line in lines:
            cols = re.split(r'\s{2,}', line.strip())
            line_column_counts.append(len(cols))

        logger.info(f"ğŸ” ê° ë¼ì¸ë³„ ì»¬ëŸ¼ ìˆ˜: {line_column_counts}")

        # ê°€ì¥ ê¸´ ì—°ì†ëœ í‘œ êµ¬ì¡° ì°¾ê¸°
        best_start = 0
        best_end = 0
        best_col_count = 0

        current_start = 0
        current_col_count = line_column_counts[0] if line_column_counts else 0

        for i in range(len(line_column_counts)):
            col_count = line_column_counts[i]

            # ì»¬ëŸ¼ ìˆ˜ê°€ 2ê°œ ë¯¸ë§Œì´ë©´ í‘œê°€ ì•„ë‹˜
            if col_count < 2:
                # ì´ì „ êµ¬ê°„ í‰ê°€
                if i - current_start >= 3 and current_col_count >= 2:
                    if i - current_start > best_end - best_start:
                        best_start = current_start
                        best_end = i
                        best_col_count = current_col_count

                # ìƒˆë¡œìš´ êµ¬ê°„ ì‹œì‘
                current_start = i + 1
                current_col_count = 0
                continue

            # ì»¬ëŸ¼ ìˆ˜ê°€ í¬ê²Œ ë³€í•˜ë©´ (Â±2 ì´ìƒ) ìƒˆë¡œìš´ êµ¬ê°„ ì‹œì‘
            if current_col_count > 0 and abs(col_count - current_col_count) > 2:
                # ì´ì „ êµ¬ê°„ í‰ê°€
                if i - current_start >= 3:
                    if i - current_start > best_end - best_start:
                        best_start = current_start
                        best_end = i
                        best_col_count = current_col_count

                # ìƒˆë¡œìš´ êµ¬ê°„ ì‹œì‘
                current_start = i
                current_col_count = col_count
            else:
                # ì»¬ëŸ¼ ìˆ˜ê°€ ë¹„ìŠ·í•˜ë©´ ê°™ì€ êµ¬ê°„
                if current_col_count == 0:
                    current_col_count = col_count

        # ë§ˆì§€ë§‰ êµ¬ê°„ í‰ê°€
        if len(line_column_counts) - current_start >= 3 and current_col_count >= 2:
            if len(line_column_counts) - current_start > best_end - best_start:
                best_start = current_start
                best_end = len(line_column_counts)
                best_col_count = current_col_count

        logger.info(f"ğŸ“Š í‘œ êµ¬ì¡° ê°ì§€: ë¼ì¸ {best_start+1}~{best_end} (ì»¬ëŸ¼ ìˆ˜: {best_col_count})")

        # í‘œ êµ¬ì¡° ì¶”ì¶œ
        table_lines = lines[best_start:best_end]

        # ì¶”ì¶œëœ ë¼ì¸ë“¤ ë¡œê¹… (ì „ì²´ ì¶œë ¥)
        logger.info(f"ğŸ“ ì¶”ì¶œëœ í‘œ êµ¬ì¡° ì „ì²´ ({len(table_lines)}ê°œ ë¼ì¸):")
        for i, line in enumerate(table_lines, start=1):
            logger.info(f"   ë¼ì¸ {i}: '{line}'")

        return table_lines

    def _parse_markdown_table(self, text: str) -> pd.DataFrame:
        """ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” íŒŒì‹±"""
        lines = [line.strip() for line in text.strip().split('\n') if line.strip() and '|' in line]

        if len(lines) < 2:
            raise ValueError("ìœ íš¨í•œ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”ì´ ì•„ë‹™ë‹ˆë‹¤")

        # í—¤ë” ì¶”ì¶œ
        headers = [col.strip() for col in lines[0].split('|')[1:-1]]  # ì–‘ë | ì œê±°

        # êµ¬ë¶„ì„  ê±´ë„ˆë›°ê¸° (|----|----| í˜•íƒœ)
        data_lines = [line for line in lines[1:] if not re.match(r'^\s*\|[\s\-\|]*\|\s*$', line)]

        data = []
        for line in data_lines:
            row = [col.strip() for col in line.split('|')[1:-1]]  # ì–‘ë | ì œê±°
            if len(row) == len(headers):
                data.append(row)

        return pd.DataFrame(data, columns=headers)

    def _detect_table_format(self, content: str) -> bool:
        """
        ì½˜í…ì¸ ê°€ í‘œ í˜•ì‹(CSV, TSV, ê³µë°± êµ¬ë¶„ ë“±)ì¸ì§€ ê°ì§€í•©ë‹ˆë‹¤.

        Args:
            content: ë¶„ì„í•  í…ìŠ¤íŠ¸ ì½˜í…ì¸ 

        Returns:
            bool: í‘œ í˜•ì‹ì´ë©´ True, ì•„ë‹ˆë©´ False
        """
        try:
            if not content or not content.strip():
                return False

            lines = content.strip().split('\n')

            # ìµœì†Œ 2ì¤„ ì´ìƒ í•„ìš” (í—¤ë” + ë°ì´í„°)
            if len(lines) < 2:
                return False

            structured_lines = 0
            for line in lines[:10]:  # ì²˜ìŒ 10ì¤„ë§Œ ê²€ì‚¬
                line = line.strip()
                if not line:
                    continue

                # êµ¬ë¶„ì íŒ¨í„´ ê°ì§€
                has_delimiter = (
                    ',' in line or      # CSV
                    '\t' in line or     # TSV
                    '|' in line or      # Markdown í…Œì´ë¸”
                    '  ' in line        # ê³µë°± êµ¬ë¶„ (2ê°œ ì´ìƒ)
                )

                if has_delimiter:
                    structured_lines += 1

            # 50% ì´ìƒì˜ ë¼ì¸ì´ êµ¬ì¡°í™”ë˜ì–´ ìˆìœ¼ë©´ í‘œ í˜•ì‹ìœ¼ë¡œ íŒë‹¨
            structure_ratio = structured_lines / min(len([l for l in lines[:10] if l.strip()]), 10)
            is_table = structure_ratio >= 0.5

            if is_table:
                logger.info(f"âœ… í‘œ í˜•ì‹ ê°ì§€: {structured_lines}ê°œ êµ¬ì¡°í™” ë¼ì¸ / êµ¬ì¡° ë¹„ìœ¨: {structure_ratio:.2f}")
            else:
                logger.info(f"âŒ í‘œ í˜•ì‹ ì•„ë‹˜: {structured_lines}ê°œ êµ¬ì¡°í™” ë¼ì¸ / êµ¬ì¡° ë¹„ìœ¨: {structure_ratio:.2f}")

            return is_table

        except Exception as e:
            logger.warning(f"âš ï¸ í‘œ í˜•ì‹ ê°ì§€ ì‹¤íŒ¨: {str(e)}")
            return False

    def _convert_numeric_columns(self, df: pd.DataFrame, numeric_cols: list) -> pd.DataFrame:
        """ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ê°ì§€ëœ ì»¬ëŸ¼ì„ numeric dtypeìœ¼ë¡œ ë³€í™˜"""
        df_converted = df.copy()

        for col in numeric_cols:
            try:
                # ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜ (errors='coerce'ë¡œ ë³€í™˜ ë¶ˆê°€ëŠ¥í•œ ê°’ì€ NaN)
                df_converted[col] = pd.to_numeric(df_converted[col], errors='coerce')
                logger.info(f"âœ… ì»¬ëŸ¼ '{col}' â†’ numeric ë³€í™˜ ì™„ë£Œ (dtype: {df_converted[col].dtype})")
            except Exception as e:
                logger.warning(f"âš ï¸ ì»¬ëŸ¼ '{col}' numeric ë³€í™˜ ì‹¤íŒ¨: {str(e)}")

        return df_converted

    def _analyze_column_types(self, df: pd.DataFrame) -> Dict[str, list]:
        """DataFrame ì»¬ëŸ¼ì˜ ë³€ìˆ˜ íƒ€ì…ì„ ë¶„ì„"""
        analysis = {
            'numeric': [],      # ìˆ˜ì¹˜í˜• (Yì¶•ìš©)
            'categorical': [],  # ë²”ì£¼í˜• (Xì¶•/ê·¸ë£¹ìš©)
            'temporal': [],     # ì‹œê³„ì—´
            'mixed': []         # í˜¼í•©í˜•
        }

        for col in df.columns:
            col_data = df[col].dropna()

            if len(col_data) == 0:
                continue

            # ì‹œê³„ì—´ í™•ì¸ì„ ë¨¼ì € ìˆ˜í–‰ (ì—°ë„ ë°ì´í„°ê°€ ìˆ«ìë¡œ ì²˜ë¦¬ë˜ëŠ” ê²ƒì„ ë°©ì§€)
            if self._is_temporal_column(col_data):
                analysis['temporal'].append(col)
                continue

            # ê³ ìœ ê°’ ë¹„ìœ¨ ê³„ì‚°
            unique_count = len(col_data.unique())
            unique_ratio = unique_count / len(col_data)

            # ìˆ˜ì¹˜í˜• í™•ì¸ (ë‹¨, '-'ëŠ” ê²°ì¸¡ê°’ìœ¼ë¡œ ì²˜ë¦¬)
            try:
                # '-'ë¥¼ NaNìœ¼ë¡œ ì¹˜í™˜ í›„ numeric ë³€í™˜ ì‹œë„
                col_data_cleaned = col_data.replace(['-', 'â€“', 'â€”'], pd.NA)  # ë‹¤ì–‘í•œ ëŒ€ì‹œ ë¬¸ì ì²˜ë¦¬
                numeric_converted = pd.to_numeric(col_data_cleaned, errors='coerce')

                # ì›ë³¸ ë°ì´í„°ì—ì„œ '-'ê°€ ì•„ë‹Œ ê°’ë“¤ ì¤‘ ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•œ ë¹„ìœ¨ ê³„ì‚°
                non_dash_data = col_data[~col_data.isin(['-', 'â€“', 'â€”'])]
                if len(non_dash_data) > 0:
                    numeric_ratio = numeric_converted.notna().sum() / len(col_data)

                    # '-'ë¥¼ ì œì™¸í•œ ê°’ë“¤ì˜ ìˆ«ì ë³€í™˜ ë¹„ìœ¨
                    if len(non_dash_data) > 0:
                        non_dash_numeric = pd.to_numeric(non_dash_data, errors='coerce')
                        non_dash_numeric_ratio = non_dash_numeric.notna().sum() / len(non_dash_data)
                    else:
                        non_dash_numeric_ratio = 0

                    # '-'ë¥¼ ì œì™¸í•œ ê°’ì˜ 50% ì´ìƒì´ ìˆ«ìë©´ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ê°„ì£¼
                    if non_dash_numeric_ratio > 0.5:
                        # ìˆ«ìì´ì§€ë§Œ ê³ ìœ ê°’ì´ ì ìœ¼ë©´ ë²”ì£¼í˜• (ì˜ˆ: ë¶„ê¸°=1,2)
                        if unique_count <= 10 and unique_ratio < 0.3:
                            analysis['categorical'].append(col)
                        else:
                            analysis['numeric'].append(col)
                        continue
            except:
                pass

            # ë²”ì£¼í˜• í™•ì¸ (ê³ ìœ ê°’ì´ ì ê³  ë°˜ë³µë˜ëŠ” íŒ¨í„´)
            if unique_ratio < 0.5 or unique_count <= 20:
                analysis['categorical'].append(col)
            else:
                analysis['mixed'].append(col)

        return analysis

    def _is_temporal_column(self, col_data) -> bool:
        """ì»¬ëŸ¼ì´ ì‹œê³„ì—´ ë°ì´í„°ì¸ì§€ í™•ì¸"""
        sample_values = col_data.astype(str).head(min(10, len(col_data)))

        # ì—°ë„ íŒ¨í„´ (2020, 2021ë…„, etc.)
        year_patterns = [
            r'^\d{4}ë…„?$',      # 2020, 2020ë…„
            r'^\d{4}-\d{2}$',   # 2020-01
            r'^\d{4}/\d{2}$'    # 2020/01
        ]

        # ì›” íŒ¨í„´ (1ì›”, Jan, January, etc.)
        month_patterns = [
            r'^\d{1,2}ì›”$',                    # 1ì›”, 12ì›”
            r'^(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)$',  # ì˜ì–´ ì›” ì¶•ì•½
        ]

        # ë‚ ì§œ íŒ¨í„´
        date_patterns = [
            r'^\d{4}-\d{2}-\d{2}$',    # 2020-01-01
            r'^\d{2}/\d{2}/\d{4}$',    # 01/01/2020
            r'^\d{1,2}/\d{1,2}$'       # 1/1, 12/31
        ]

        all_patterns = year_patterns + month_patterns + date_patterns

        # ì—°ë„ ë²”ìœ„ í™•ì¸ (1900-2100 ì‚¬ì´ì˜ 4ìë¦¬ ìˆ«ì)
        year_range_matches = 0
        for value in sample_values:
            try:
                num_val = int(str(value).strip())
                if 1900 <= num_val <= 2100:
                    year_range_matches += 1
            except:
                pass

        # íŒ¨í„´ ë§¤ì¹­ í™•ì¸
        pattern_matches = 0
        for value in sample_values:
            if any(re.match(pattern, str(value).strip()) for pattern in all_patterns):
                pattern_matches += 1

        # íŒ¨í„´ì´ 70% ì´ìƒ ë§¤ì¹˜ë˜ê±°ë‚˜, ì—°ë„ ë²”ìœ„ê°€ 80% ì´ìƒ ë§¤ì¹˜ë˜ë©´ ì‹œê³„ì—´ë¡œ íŒë‹¨
        pattern_ratio = pattern_matches / len(sample_values) if len(sample_values) > 0 else 0
        year_ratio = year_range_matches / len(sample_values) if len(sample_values) > 0 else 0

        is_temporal = pattern_ratio > 0.7 or year_ratio > 0.8

        return is_temporal

    def _format_dataframe_for_llm(self, df: pd.DataFrame, viz_type: VisualizationType, aggregation_info: str = None) -> str:
        """ë°ì´í„°í”„ë ˆì„ì„ LLMì´ ì´í•´í•˜ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""

        # ë³€ìˆ˜ íƒ€ì… ë¶„ì„
        column_analysis = self._analyze_column_types(df)

        # ê¸°ë³¸ ë°ì´í„° ì •ë³´
        info = f"ë°ì´í„° ê°œìš”:\n"
        info += f"- í–‰ ìˆ˜: {len(df)}\n"
        info += f"- ì—´ ìˆ˜: {len(df.columns)}\n"
        info += f"- ì—´ ì´ë¦„: {', '.join(df.columns)}\n"

        # ì§‘ê³„ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€
        if aggregation_info:
            info += f"- ë°ì´í„° ì§‘ê³„: {aggregation_info}\n"
        info += "\n"

        # ë³€ìˆ˜ íƒ€ì… ì •ë³´
        info += "ë³€ìˆ˜ íƒ€ì… ë¶„ì„:\n"
        info += f"- ë²”ì£¼í˜• ë³€ìˆ˜ (Xì¶•/ê·¸ë£¹ìš©): {column_analysis['categorical']}\n"
        info += f"- ìˆ˜ì¹˜í˜• ë³€ìˆ˜ (Yì¶•ìš©): {column_analysis['numeric']}\n"
        info += f"- ì‹œê³„ì—´ ë³€ìˆ˜: {column_analysis['temporal']}\n\n"

        # ì‹œê°í™” ìœ í˜•ì— ë”°ë¥¸ ë°ì´í„° í˜•ì‹
        if viz_type == VisualizationType.TABLE:
            # í‘œëŠ” ì „ì²´ ë°ì´í„° í¬í•¨
            formatted_data = info + "ì „ì²´ ë°ì´í„°:\n" + df.to_string(index=False)

        elif viz_type == VisualizationType.CHART:
            # ì°¨íŠ¸ëŠ” ìˆ˜ì¹˜ ë°ì´í„° ì¤‘ì‹¬ + ì‹¤ì œ ì „ì²´ ë°ì´í„° í¬í•¨
            formatted_data = info

            # ì‹œê°í™” ê°€ì´ë“œ ì¶”ê°€
            formatted_data += "ì°¨íŠ¸ ì‹œê°í™” ê°€ì´ë“œ:\n"
            if column_analysis['numeric']:
                formatted_data += f"- Yì¶•ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ì¹˜í˜• ë³€ìˆ˜: {column_analysis['numeric']}\n"
            if column_analysis['categorical']:
                formatted_data += f"- Xì¶•/ê·¸ë£¹ìœ¼ë¡œ ì‚¬ìš©í•  ë²”ì£¼í˜• ë³€ìˆ˜: {column_analysis['categorical']}\n"
            if column_analysis['temporal']:
                formatted_data += f"- ì‹œê³„ì—´ ì¶•ìœ¼ë¡œ ì‚¬ìš©í•  ë³€ìˆ˜: {column_analysis['temporal']}\n"

            # ì§‘ê³„ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° ì°¨íŠ¸ ì œëª© ìƒì„± ê°€ì´ë“œ ì¶”ê°€
            if aggregation_info:
                if "í‰ê· " in aggregation_info or "mean" in aggregation_info.lower():
                    formatted_data += f"- ì°¨íŠ¸ ì œëª©ì— 'í‰ê· 'ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš” (ì˜ˆ: 'ì—°ë„ë³„ í‰ê·  íŒë§¤ëŸ‰ ì¶”ì´')\n"
                elif "í•©ê³„" in aggregation_info or "sum" in aggregation_info.lower():
                    formatted_data += f"- ì°¨íŠ¸ ì œëª©ì— 'í•©ê³„' ë˜ëŠ” 'ì´'ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš” (ì˜ˆ: 'ì œí’ˆë³„ ì´ íŒë§¤ëŸ‰')\n"
            formatted_data += "\n"

            # ì‹¤ì œ ì „ì²´ ë°ì´í„° í¬í•¨ (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ë¥¼ ìœ„í•´ í•„ìˆ˜)
            formatted_data += "ì „ì²´ ì‹¤ì œ ë°ì´í„° (ë°˜ë“œì‹œ ì´ ë°ì´í„°ë§Œ ì‚¬ìš©):\n" + df.to_string(index=False)

        return formatted_data

    async def _structure_content_to_table(self, content: str, content_type: str, viz_type: VisualizationType) -> str:
        """
        PDF, ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ í‘œ/CSV í˜•ì‹ìœ¼ë¡œ ì§ì ‘ êµ¬ì¡°í™”

        Args:
            content: ì›ë³¸ ì½˜í…ì¸ 
            content_type: ì½˜í…ì¸  íƒ€ì… ("pdf", "image", "text")
            viz_type: ì‹œê°í™” íƒ€ì… (TABLE ë˜ëŠ” CHART)

        Returns:
            str: CSV í˜•ì‹ì˜ êµ¬ì¡°í™”ëœ ë°ì´í„°
        """
        try:
            # ì½˜í…ì¸  íƒ€ì…ë³„ í‘œ êµ¬ì¡°í™” í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._build_table_structuring_prompt(content, content_type, viz_type)

            # LLMìœ¼ë¡œ í‘œ/CSV êµ¬ì¡°í™”
            from app.core.config import settings
            structured_response = await call_llm(
                prompt=prompt,
                model=settings.llm_visualization_model
            )

            # call_llm ì‘ë‹µ ì²˜ë¦¬
            if hasattr(structured_response, 'content'):
                structured_content = structured_response.content
            elif isinstance(structured_response, str):
                structured_content = structured_response
            else:
                structured_content = str(structured_response)

            logger.info(f"í‘œ í˜•íƒœ êµ¬ì¡°í™” ì™„ë£Œ: {len(structured_content)} ë¬¸ì")
            return structured_content

        except Exception as e:
            logger.error(f"í‘œ êµ¬ì¡°í™” ì‹¤íŒ¨: {str(e)}")
            # êµ¬ì¡°í™” ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì½˜í…ì¸  ë°˜í™˜
            return content

    def _build_table_structuring_prompt(self, content: str, content_type: str, viz_type: VisualizationType) -> str:
        """ì½˜í…ì¸  íƒ€ì…ë³„ í‘œ/CSV êµ¬ì¡°í™” í”„ë¡¬í”„íŠ¸ ìƒì„±"""

        # ì‹œê°í™” íƒ€ì…ì— ë”°ë¥¸ ê¸°ë³¸ ì§€ì‹œì‚¬í•­
        if viz_type == VisualizationType.TABLE:
            base_instruction = (
                "ì£¼ì–´ì§„ ì½˜í…ì¸ ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ **í‘œ(CSV) í˜•ì‹**ìœ¼ë¡œ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”.\n\n"
                "ì¶œë ¥ í˜•ì‹:\n"
                "- ì²« ë²ˆì§¸ ì¤„: ì»¬ëŸ¼ëª…1,ì»¬ëŸ¼ëª…2,ì»¬ëŸ¼ëª…3...\n"
                "- ì´í›„ ì¤„: ë°ì´í„°1,ë°ì´í„°2,ë°ì´í„°3...\n"
                "- ëª¨ë“  ì…€ì— ë‚´ìš© í•„ìˆ˜ (ë¹ˆ ì…€ì€ '-'ë¡œ í‘œì‹œ)\n"
                "- ì‰¼í‘œê°€ í¬í•¨ëœ ë‚´ìš©ì€ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´\n\n"
                "ì¤‘ìš”: ë¬¸ì„œì— ëª…ì‹œëœ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ê³ , ì¶”ê°€ ê³„ì‚°ì´ë‚˜ ìƒˆë¡œìš´ ì»¬ëŸ¼ì„ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”."
            )
        else:  # CHART
            base_instruction = (
                "ì£¼ì–´ì§„ ì½˜í…ì¸ ì—ì„œ ì°¨íŠ¸ë¡œ ì‹œê°í™”í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ **CSV í˜•ì‹**ìœ¼ë¡œ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”.\n\n"
                "ì¶œë ¥ í˜•ì‹:\n"
                "- ì²« ë²ˆì§¸ ì¤„: ì»¬ëŸ¼ëª…1,ì»¬ëŸ¼ëª…2\n"
                "- ì´í›„ ì¤„: í•­ëª©,ê°’ (ì˜ˆ: 2020ë…„,3500)\n"
                "- ìˆ˜ì¹˜ ë°ì´í„°ëŠ” ìˆœìˆ˜ ìˆ«ìë§Œ ì…ë ¥ (ë‹¨ìœ„ ì œê±°)\n"
                "- ëª¨ë“  ì…€ì— ë‚´ìš© í•„ìˆ˜\n\n"
                "ì¤‘ìš”: ë¬¸ì„œì— ëª…ì‹œëœ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ê³ , ê³„ì‚°ì´ë‚˜ ì¶”ì •ì€ í•˜ì§€ ë§ˆì„¸ìš”."
            )

        content_specific_prompts = {
            "pdf": (
                f"{base_instruction}\n\n"
                "PDF ë¬¸ì„œì—ì„œ í‘œ, ë°ì´í„°, í†µê³„ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ CSVë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.\n\n"
                f"PDF ë‚´ìš©:\n{content}"
            ),
            "image": (
                f"{base_instruction}\n\n"
                "ì´ë¯¸ì§€ì—ì„œ í‘œ, ì°¨íŠ¸, ê·¸ë˜í”„ì˜ ë°ì´í„°ë¥¼ ì½ì–´ì„œ CSVë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.\n\n"
                f"ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼:\n{content}"
            ),
            "text": (
                f"{base_instruction}\n\n"
                "í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™” ê°€ëŠ¥í•œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ CSVë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.\n\n"
                f"í…ìŠ¤íŠ¸ ë‚´ìš©:\n{content}"
            )
        }

        return content_specific_prompts.get(content_type, content_specific_prompts["text"])

    async def upload_file_to_ncp(self, file_path: str, base_name: str) -> str:
        """
        ë¡œì»¬ íŒŒì¼ì„ NCP VISUAL ë²„í‚·ì— ì—…ë¡œë“œ

        Args:
            file_path: ì—…ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ
            base_name: íŒŒì¼ëª…ì˜ ë² ì´ìŠ¤ ì´ë¦„

        Returns:
            str: ì—…ë¡œë“œëœ íŒŒì¼ì˜ NCP URL
        """
        try:
            # ë²„í‚·ëª… í™•ì¸ ë° ë””ë²„ê¹…
            bucket_name = settings.naver_bucket_name
            visual_bucket_name = settings.naver_bucket_visual

            if not bucket_name:
                raise ValueError("NAVER_BUCKET_VISUAL í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            # ë¡œì»¬ íŒŒì¼ëª… ê·¸ëŒ€ë¡œ ì‚¬ìš© (ê²½ë¡œì—ì„œ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ)
            filename = os.path.basename(file_path)

            date_folder = datetime.now().strftime("%Y%m%d")
            ncp_path = f"{visual_bucket_name}/{date_folder}/{filename}" 

            # ë¡œì»¬ íŒŒì¼ì„ NCPì— ì—…ë¡œë“œ
            with open(file_path, 'rb') as file_data:
                self.s3_client.upload_fileobj(
                    file_data,
                    bucket_name,
                    ncp_path,
                )

                # íŒŒì¼ì„ ê³µê°œë¡œ ì„¤ì •
                self.s3_client.put_object_acl(
                    Bucket=settings.naver_bucket_name, 
                    Key=ncp_path, 
                    ACL='public-read'
                )

            # NCP URL ìƒì„± (ì—”ë“œí¬ì¸íŠ¸ ì œì™¸í•˜ê³  ê²½ë¡œë§Œ ë°˜í™˜)
            ncp_url = f"{bucket_name}/{visual_bucket_name}/{date_folder}/{filename}"

            logger.info(f"íŒŒì¼ NCP ì—…ë¡œë“œ ì™„ë£Œ: {ncp_url}")
            return ncp_url

        except Exception as e:
            logger.error(f"NCP ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            raise ValueError(f"íŒŒì¼ì„ NCP ë²„í‚·ì— ì—…ë¡œë“œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

    async def analyze_visualizations_with_tts(
        self,
        ncp_url: List[str],
        model: str = None,
        language: str = "ko"
    ) -> Dict[str, Any]:
        """
        ì‹œê°í™” ì´ë¯¸ì§€ë“¤ì„ ë¶„ì„í•˜ê³  TTSë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        Args:
            ncp_url: ë¶„ì„í•  ì‹œê°í™” ì´ë¯¸ì§€ NCP URL ëª©ë¡
            model: ì‚¬ìš©í•  LLM ëª¨ë¸
            language: ë¶„ì„ ì–¸ì–´ (ko, en, ja, zh ë“±)

        Returns:
            Dict: ë¶„ì„ ê²°ê³¼ ëª©ë¡
        """
        try:
            if not model:
                model = settings.llm_visualization_model

            analyses = []

            for viz_url in ncp_url:
                try:
                    # 1. NCP URLì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                    image_bytes = await self._download_from_ncp(viz_url)

                    # 2. LLMìœ¼ë¡œ ì‹œê°í™” ì´ë¯¸ì§€ ë¶„ì„ (ì–¸ì–´ë³„)
                    analysis_text = await self._analyze_visualization_image(image_bytes, model, language)

                    # 3. ë¶„ì„ í…ìŠ¤íŠ¸ë¡œ TTS ìƒì„± (ì–¸ì–´ë³„)
                    tts_url = await self._generate_tts_for_analysis(analysis_text, language)

                    analyses.append({
                        "analysis_text": analysis_text,
                        "ncp_url": tts_url
                    })
                    
                    logger.info(f"âœ… ì‹œê°í™” ë¶„ì„ ì™„ë£Œ: {viz_url}")

                except Exception as e:
                    logger.error(f"âŒ ì‹œê°í™” ë¶„ì„ ì‹¤íŒ¨ ({viz_url}): {str(e)}")
                    continue

            return {
                "success": True,
                "analyses": analyses
            }

        except Exception as e:
            logger.error(f"ì‹œê°í™” ë¶„ì„ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }

    async def _download_from_ncp(self, ncp_url: str) -> bytes:
        """NCP URLì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
        try:
            import httpx

            # ì „ì²´ URL êµ¬ì„±
            full_url = f"{settings.naver_endpoint_url}/{ncp_url}"

            logger.info(f"ğŸ”½ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œë„: {full_url}")

            async with httpx.AsyncClient() as client:
                response = await client.get(full_url)
                response.raise_for_status()

                logger.info(f"âœ… ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(response.content)} bytes")
                return response.content

        except Exception as e:
            logger.error(f"NCP ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            raise ValueError(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")


    async def _analyze_visualization_image(self, image_bytes: bytes, model: str, language: str = None) -> str:
        """LLMìœ¼ë¡œ ì‹œê°í™” ì´ë¯¸ì§€ ë¶„ì„"""
        try:
            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            base64_image = base64.b64encode(image_bytes).decode('utf-8')

            # ì´ë¯¸ì§€ í˜•ì‹ í™•ì¸
            from PIL import Image
            image_file = io.BytesIO(image_bytes)
            image = Image.open(image_file)
            image_format = image.format.lower() if image.format else 'png'

            # ì–¸ì–´ë³„ ì‹œê°í™” ë¶„ì„ í”„ë¡¬í”„íŠ¸ (ëª¨ë“ˆí™”)
            prompt = get_analysis_prompt(language)

            # LangChain HumanMessageë¥¼ ì‚¬ìš©í•œ ë©€í‹°ëª¨ë‹¬ ë©”ì‹œì§€ ìƒì„±
            message_content = [
                {
                    "type": "text",
                    "text": prompt
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/{image_format};base64,{base64_image}"
                    }
                }
            ]

            response = await call_llm(
                prompt=[{
                    "role": "user",
                    "content": message_content
                }],
                model=model,
                max_tokens=settings.openai_max_tokens
            )
            response_text = response.content if response and response.content else ""

            if not response_text.strip():
                raise Exception("ì‹œê°í™” ë¶„ì„ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

            # JSON íŒŒì‹±í•˜ì—¬ analysis_text í•„ë“œë§Œ ì¶”ì¶œ
            try:
                import json
                import re

                # JSON ì¶”ì¶œ ì‹œë„
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    parsed = json.loads(json_match.group(0))
                    analysis_text = parsed.get("analysis_text", response_text)
                else:
                    # JSONì´ ì•„ë‹Œ ê²½ìš° ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©
                    analysis_text = response_text

            except Exception as parse_error:
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨, ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©: {str(parse_error)}")
                analysis_text = response_text

            logger.info(f"âœ… ì‹œê°í™” ë¶„ì„ ì™„ë£Œ: {len(analysis_text)} ë¬¸ì")
            return analysis_text.strip()

        except Exception as e:
            logger.error(f"ì‹œê°í™” ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

    async def _generate_tts_for_analysis(self, analysis_text: str, language: str = "ko") -> str:
        """ë¶„ì„ í…ìŠ¤íŠ¸ë¡œ TTS ìƒì„± ë° NCP ì—…ë¡œë“œ"""
        try:
            from app.services.voice.tts.generator import TTSService
            from app.models.voice.tts import SingleTTSRequest, GenderType

            # TTS ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
            tts_service = TTSService()

            # TTS ìƒì„± ìš”ì²­ (configì˜ default_tts_provider ì‚¬ìš©)
            tts_request = SingleTTSRequest(
                text=analysis_text,
                voice=None,  # providerì— ë”°ë¼ ìë™ ì„ íƒ
                gender_hint=GenderType.MALE
            )

            # TTS ìƒì„±
            tts_response = await tts_service.generate_single_tts(tts_request)

            if not tts_response.success or not tts_response.ncp_url:
                raise Exception("TTS ìƒì„± ì‹¤íŒ¨")

            logger.info(f"âœ… TTS ìƒì„± ì™„ë£Œ: {tts_response.ncp_url}")
            return tts_response.ncp_url

        except Exception as e:
            logger.error(f"TTS ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"TTS ìƒì„± ì‹¤íŒ¨: {str(e)}")
