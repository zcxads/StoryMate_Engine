from typing import Dict, Any, Optional
from urllib.parse import urlparse

from app.utils.logger.setup import setup_logger
from app.utils.language.generator import call_llm
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import StateGraph, END
from langchain_core.runnables import RunnablePassthrough

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import requests

# ì„¤ì • ë° í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ import
from app.core.config import settings
from app.prompts.main_crawler.generator import get_content_extraction_prompt
from app.services.main_crawler.naver_web_crawler import NaverWebCrawler
from app.services.language.language_detection.detector import detect_language_with_ai

import re
import time

logger = setup_logger('main_crawler')

class MainCrawlerAgent:
    """URL ë³¸ë¬¸ ì¶”ì¶œ ì—ì´ì „íŠ¸ - LangChain/LangGraph ê¸°ë°˜"""

    # ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
    _instance = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, llm_api_key: str = None):
        # ì´ë¯¸ ì´ˆê¸°í™”ëœ ê²½ìš° ì¤‘ë³µ ì´ˆê¸°í™” ë°©ì§€
        if hasattr(self, '_initialized'):
            return
        self._initialized = True

        # API í‚¤ ì„¤ì • (ë§¤ê°œë³€ìˆ˜ ìš°ì„ , ì—†ìœ¼ë©´ ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        self.openai_api_key = llm_api_key or settings.openai_api_key

        # Selenium WebDriver ì„¤ì •
        self._selenium_driver = None
        self._setup_selenium_driver()

        # í¬ë¡¤ë§ í†µê³„
        self.crawl_stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_response_time": 0.0,
            "total_content_length": 0
        }

        # ëª¨ë¸ëª… ì €ì¥ (call_llm ì‚¬ìš©)
        self.model = settings.web_crawler_model
        self.setup_workflow()
    
    @classmethod
    def get_instance(cls):
        """ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
    
    @classmethod
    def cleanup(cls):
        """ì •ì  cleanup ë©”ì„œë“œ - ì „ì—­ WebDriver ì •ë¦¬"""
        if cls._instance and cls._instance._selenium_driver:
            try:
                cls._instance._close_selenium_driver()
                logger.info("ğŸ§¹ ì „ì—­ Selenium WebDriver ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âš ï¸ ì „ì—­ Selenium WebDriver ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        else:
            logger.info("â„¹ï¸ ì •ë¦¬í•  WebDriver ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
    
    def _setup_selenium_driver(self):
        """Selenium WebDriver ì„¤ì •"""
        try:
            chrome_options = Options()

            # ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¨ ì˜µì…˜ ì ìš©
            if settings.web_crawler_headless:
                chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument(f'--window-size={settings.web_crawler_window_size}')
            chrome_options.add_argument(f'--user-agent={settings.web_crawler_user_agent}')
            
            # ìë™í™” ê°ì§€ ë°©ì§€
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            service = Service(ChromeDriverManager().install())
            self._selenium_driver = webdriver.Chrome(service=service, options=chrome_options)
            
            # ìë™í™” ê°ì§€ ë°©ì§€ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
            self._selenium_driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
        except Exception as e:
            logger.error(f"Selenium WebDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            self._selenium_driver = None
    
    def _close_selenium_driver(self):
        """Selenium WebDriver ì¢…ë£Œ"""
        if self._selenium_driver:
            try:
                self._selenium_driver.quit()
                self._selenium_driver = None
            except Exception as e:
                logger.error(f"Selenium WebDriver ì¢…ë£Œ ì‹¤íŒ¨: {str(e)}")
    
    def _handle_302_redirect(self, url: str) -> str:
        """302 ì—ëŸ¬ ì²˜ë¦¬ ë° ë¦¬ë‹¤ì´ë ‰íŠ¸ URL ë°˜í™˜"""
        try:
            logger.info(f"ğŸ”„ 302 ë¦¬ë‹¤ì´ë ‰íŠ¸ ì²´í¬: {url}")
            
            # requestsë¥¼ ì‚¬ìš©í•˜ì—¬ 302 ë¦¬ë‹¤ì´ë ‰íŠ¸ ì²´í¬
            headers = {
                'User-Agent': settings.web_crawler_user_agent,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            
            # ë¦¬ë‹¤ì´ë ‰íŠ¸ í—ˆìš©í•˜ì—¬ ìš”ì²­
            response = requests.get(
                url, 
                headers=headers, 
                allow_redirects=True, 
                timeout=10,
                verify=False
            )
            
            final_url = response.url
            status_code = response.status_code
            
            # 302 ì—ëŸ¬ì¸ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
            if status_code == 302:
                logger.info(f"ğŸ”„ 302 ë¦¬ë‹¤ì´ë ‰íŠ¸ ê°ì§€: {url} â†’ {final_url}")
                return final_url
            else:
                logger.info(f"ğŸ“Š HTTP ìƒíƒœ ì½”ë“œ: {status_code} (302 ì•„ë‹˜)")
                return url
                
        except requests.exceptions.TooManyRedirects:
            logger.error(f"âŒ ë„ˆë¬´ ë§ì€ ë¦¬ë‹¤ì´ë ‰íŠ¸: {url}")
            return url
            
        except requests.exceptions.Timeout:
            logger.error(f"âŒ 302 ì²´í¬ íƒ€ì„ì•„ì›ƒ: {url}")
            return url
            
        except requests.exceptions.ConnectionError:
            logger.error(f"âŒ ì—°ê²° ì˜¤ë¥˜: {url}")
            return url
            
        except Exception as e:
            logger.error(f"âŒ 302 ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return url
    
    async def _crawl_website(self, url: str) -> tuple[str, str]:
        """ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ - ê°œì„ ëœ timeout ì²˜ë¦¬"""
        self.crawl_stats["total_requests"] += 1
        
        max_retries = 2  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        retry_count = 0
        
        while retry_count <= max_retries:
            try:
                if not self._selenium_driver:
                    self._setup_selenium_driver()
                
                # 1ë‹¨ê³„: 302 ì—ëŸ¬ ì²´í¬ ë° ì²˜ë¦¬
                final_url = self._handle_302_redirect(url)
                logger.info(f"ğŸŒ ìµœì¢… URL: {final_url}")
                
                # 2ë‹¨ê³„: í˜ì´ì§€ ë¡œë“œ (ì¬ì‹œë„ ì‹œ ë” ì§§ì€ timeout)
                current_timeout = settings.web_crawler_timeout - (retry_count * 10)  # ì¬ì‹œë„ ì‹œ timeout ë‹¨ì¶•
                current_timeout = max(current_timeout, 15)  # ìµœì†Œ 15ì´ˆ ë³´ì¥
                
                logger.info(f"ğŸŒ í˜ì´ì§€ ë¡œë“œ ì‹œì‘: {final_url} (timeout: {current_timeout}ì´ˆ, ì‹œë„: {retry_count + 1}/{max_retries + 1})")
                self._selenium_driver.set_page_load_timeout(current_timeout)
                self._selenium_driver.get(final_url)
                
                # 3ë‹¨ê³„: í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ì¬ì‹œë„ ì‹œ ë” ì§§ì€ ëŒ€ê¸°ì‹œê°„)
                wait_timeout = 15 - (retry_count * 3)  # ì¬ì‹œë„ ì‹œ ëŒ€ê¸°ì‹œê°„ ë‹¨ì¶•
                wait_timeout = max(wait_timeout, 5)  # ìµœì†Œ 5ì´ˆ ë³´ì¥
                
                try:
                    WebDriverWait(self._selenium_driver, wait_timeout).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )
                    logger.info(f"âœ… í˜ì´ì§€ ë¡œë”© ì™„ë£Œ (ëŒ€ê¸°ì‹œê°„: {wait_timeout}ì´ˆ)")
                except TimeoutException:
                    if retry_count < max_retries:
                        logger.warning(f"âš ï¸ í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ (ëŒ€ê¸°ì‹œê°„: {wait_timeout}ì´ˆ), ì¬ì‹œë„ ì˜ˆì •")
                        retry_count += 1
                        continue
                    else:
                        logger.warning("âš ï¸ í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ, ê³„ì† ì§„í–‰")

                # 4ë‹¨ê³„: ì œëª© ì¶”ì¶œ
                title = self._selenium_driver.title
                
                # 5ë‹¨ê³„: ë³¸ë¬¸ ì¶”ì¶œ
                content = await self._extract_content()
                
                if content and len(content) > 0:
                    self.crawl_stats["successful_requests"] += 1
                    logger.info(f"âœ… í¬ë¡¤ë§ ì„±ê³µ: {len(content)}ì")
                    return content, title
                else:
                    if retry_count < max_retries:
                        logger.warning(f"âš ï¸ ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨, ì¬ì‹œë„ ì˜ˆì • (ì‹œë„: {retry_count + 1}/{max_retries + 1})")
                        retry_count += 1
                        continue
                    else:
                        self.crawl_stats["failed_requests"] += 1
                        logger.error("âŒ í¬ë¡¤ë§ ì‹¤íŒ¨")
                        return "", title
            
            except WebDriverException as e:
                error_msg = str(e)
                
                # WebDriver ê´€ë ¨ ì˜¤ë¥˜ ì²˜ë¦¬
                if "ERR_NAME_NOT_RESOLVED" in error_msg:
                    logger.error(f"âŒ DNS í•´ì„ ì‹¤íŒ¨: {url}")
                    return "", ""
                elif "ERR_CONNECTION_REFUSED" in error_msg:
                    logger.error(f"âŒ ì—°ê²° ê±°ë¶€ë¨: {url}")
                    return "", ""
                elif "ERR_CONNECTION_TIMED_OUT" in error_msg:
                    if retry_count < max_retries:
                        logger.warning(f"âš ï¸ ì—°ê²° íƒ€ì„ì•„ì›ƒ, ì¬ì‹œë„ ì˜ˆì •: {url}")
                        retry_count += 1
                        continue
                    else:
                        logger.error(f"âŒ ì—°ê²° íƒ€ì„ì•„ì›ƒ (ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼): {url}")
                        self.crawl_stats["failed_requests"] += 1
                        return "", ""
                elif "ERR_SSL_PROTOCOL_ERROR" in error_msg:
                    logger.error(f"âŒ SSL í”„ë¡œí† ì½œ ì˜¤ë¥˜: {url}")
                    return "", ""
                elif "ERR_CERT_AUTHORITY_INVALID" in error_msg:
                    logger.error(f"âŒ SSL ì¸ì¦ì„œ ì˜¤ë¥˜: {url}")
                    return "", ""
                else:
                    if retry_count < max_retries:
                        logger.warning(f"âš ï¸ WebDriver ì˜¤ë¥˜, ì¬ì‹œë„ ì˜ˆì •: {error_msg}")
                        retry_count += 1
                        continue
                    else:
                        logger.error(f"âŒ WebDriver ì˜¤ë¥˜ (ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼): {error_msg}")
                        self.crawl_stats["failed_requests"] += 1
                        return "", ""
                    
            except TimeoutException as e:
                if retry_count < max_retries:
                    logger.warning(f"âš ï¸ í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ, ì¬ì‹œë„ ì˜ˆì •: {url}")
                    retry_count += 1
                    continue
                else:
                    logger.error(f"âŒ í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ (ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼): {url}")
                    self.crawl_stats["failed_requests"] += 1
                    return "", ""
                    
            except Exception as e:
                if retry_count < max_retries:
                    logger.warning(f"âš ï¸ í¬ë¡¤ë§ ì˜¤ë¥˜, ì¬ì‹œë„ ì˜ˆì •: {str(e)}")
                    retry_count += 1
                    continue
                else:
                    logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨ (ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼): {str(e)}")
                    self.crawl_stats["failed_requests"] += 1
                    return "", ""
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ
        logger.error(f"âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {url}")
        self.crawl_stats["failed_requests"] += 1
        return "", ""
    
    async def _extract_content(self) -> str:
        """ë³¸ë¬¸ ì¶”ì¶œ"""
        extract_start = time.time()
        try:
            current_url = self._selenium_driver.current_url
            
            # ë„¤ì´ë²„ ë¸”ë¡œê·¸/ì¹´í˜ë§Œ íŠ¹í™” í¬ë¡¤ëŸ¬ ì‚¬ìš©
            if 'blog.naver.com' in current_url or 'cafe.naver.com' in current_url:
                logger.info("ğŸ” ë„¤ì´ë²„ ë¸”ë¡œê·¸/ì¹´í˜ ê°ì§€, NaverWebCrawler ì‚¬ìš©")
                naver_crawler = NaverWebCrawler(self._selenium_driver, self.model)
                naver_content = await naver_crawler.extract_naver_content()
                if naver_content:
                    extract_time = time.time() - extract_start
                    logger.info(f"â±ï¸ ë„¤ì´ë²„ í¬ë¡¤ëŸ¬ ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ: {extract_time:.2f}ì´ˆ")
                    return naver_content
            
            # ê·¸ ì™¸ ëª¨ë“  ì‚¬ì´íŠ¸(ë„¤ì´ë²„ ë‰´ìŠ¤ í¬í•¨)ëŠ” ì¼ë°˜ ì‚¬ì´íŠ¸ë¡œ ì²˜ë¦¬
            logger.info("ğŸŒ ì¼ë°˜ ì‚¬ì´íŠ¸ë¡œ ì²˜ë¦¬")
            result = await self._extract_general_content()
            extract_time = time.time() - extract_start
            logger.info(f"â±ï¸ LLM ê¸°ë°˜ ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ: {extract_time:.2f}ì´ˆ")
            return result
            
        except Exception as e:
            extract_time = time.time() - extract_start
            logger.error(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({extract_time:.2f}ì´ˆ): {str(e)}")
            return ""
    
    async def _extract_general_content(self) -> str:
        """ì¼ë°˜ ì‚¬ì´íŠ¸ ë³¸ë¬¸ ì¶”ì¶œ - LLM í™œìš©"""
        try:
            # 1ë‹¨ê³„: HTML í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
            html_content = self._selenium_driver.page_source
            
            if not html_content or len(html_content) < 100:
                logger.warning(f"âš ï¸ í˜ì´ì§€ ì†ŒìŠ¤ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìŒ: {len(html_content) if html_content else 0}ì")
                return ""
            
            logger.info(f"ğŸ” [EXTRACT] HTML ì†ŒìŠ¤ ê¸¸ì´: {len(html_content)}ì")
            
            # 2ë‹¨ê³„: LLMì—ê²Œ HTMLì„ ì „ë‹¬í•´ì„œ ë³¸ë¬¸ë§Œ ì¶”ì¶œ
            try:
                extracted_content = await self._extract_content_with_llm(html_content)
                
                if extracted_content and len(extracted_content.strip()) > 30:
                    logger.info(f"âœ… [EXTRACT] LLM ì¶”ì¶œ ì„±ê³µ: {len(extracted_content)}ì")
                    return self._clean_content(extracted_content)
                else:
                    logger.warning(f"âš ï¸ [EXTRACT] LLM ì¶”ì¶œ ê²°ê³¼ê°€ ë¶€ì¡±: {len(extracted_content) if extracted_content else 0}ì")
                    
            except Exception as llm_e:
                logger.error(f"âŒ [EXTRACT] LLM ì¶”ì¶œ ì‹¤íŒ¨: {str(llm_e)}")
                
        except Exception as e:
            logger.error(f"âŒ [EXTRACT] ì „ì²´ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return ""
    
    async def _extract_content_with_llm(self, html_content: str) -> str:
        """ì „ì²´ HTMLì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ + ë©”íƒ€ë°ì´í„° ì œê±° (í†µí•©, ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸)"""
        try:
            logger.info("ğŸ”„ HTML ì •ë¦¬ ë° ì–¸ì–´ ê°ì§€ ì¤‘...")

            # 1ë‹¨ê³„: BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±
            soup = BeautifulSoup(html_content, 'html.parser')

            # 2ë‹¨ê³„: ì•ˆì „í•œ íƒœê·¸ ì œê±° (ë³¸ë¬¸ ë³´ì¡´)
            self._remove_unnecessary_tags(soup)

            # 3ë‹¨ê³„: ì •ë¦¬ëœ HTMLì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            cleaned_text = soup.get_text(separator='\n', strip=True)

            logger.info(f"âœ… HTML ì •ë¦¬ ì™„ë£Œ (ì›ë³¸: {len(html_content)}ì â†’ ì •ë¦¬: {len(cleaned_text)}ì)")

            # 4ë‹¨ê³„: í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì›ë³¸ ì‚¬ìš© (ì•ˆì „ì¥ì¹˜)
            if len(cleaned_text) < 100:
                logger.warning("âš ï¸ ì •ë¦¬ëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì ìŒ, ì›ë³¸ HTML ì‚¬ìš©")
                cleaned_text = soup.get_text(separator='\n', strip=True)

            # 5ë‹¨ê³„: AI ê¸°ë°˜ ì–¸ì–´ ê°ì§€
            detection_result = await detect_language_with_ai(cleaned_text)
            lang_code = detection_result.get("primary_language")
            confidence = detection_result.get("confidence", 0.0)
            logger.info(f"ğŸŒ AI ì–¸ì–´ ê°ì§€: {lang_code}, ì‹ ë¢°ë„: {confidence:.2f}")

            # 6ë‹¨ê³„: ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
            prompt_template = get_content_extraction_prompt(lang_code)
            formatted_prompt = prompt_template.format(raw_content=cleaned_text)

            messages = [{"role": "user", "content": formatted_prompt}]

            # 7ë‹¨ê³„: LLMìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ + ë©”íƒ€ë°ì´í„° ì œê±° (1ë²ˆë§Œ í˜¸ì¶œ)
            response = await call_llm(prompt=messages, model=self.model)
            extracted_text = response.content.strip()

            logger.info(f"âœ… LLM ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ ({len(extracted_text)}ì, ì–¸ì–´: {lang_code})")

            return extracted_text

        except Exception as e:
            logger.error(f"âŒ LLM ë³¸ë¬¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return ""
    
    def _remove_unnecessary_tags(self, soup: BeautifulSoup):
        """ë¶ˆí•„ìš”í•œ HTML íƒœê·¸ë“¤ ì œê±°"""
        try:
            # 1. ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ë©”íƒ€ íƒœê·¸ë§Œ ì œê±° (ì•ˆì „í•œ íƒœê·¸ë“¤)
            for tag in soup.find_all(['script', 'style', 'meta', 'link', 'noscript']):
                tag.decompose()
            
            # 2. í¼ ê´€ë ¨ íƒœê·¸ ì œê±° (ê²€ìƒ‰ì°½, ë¡œê·¸ì¸ í¼ ë“±)
            for tag in soup.find_all(['form', 'input', 'button', 'select', 'textarea']):
                tag.decompose()
            
            # 3. ê´‘ê³  ê´€ë ¨ íƒœê·¸ ì œê±°
            for tag in soup.find_all(['iframe', 'embed', 'object']):
                tag.decompose()
            
            # 4. ëª…í™•íˆ ë¶ˆí•„ìš”í•œ í´ë˜ìŠ¤ëª…ë§Œ ì œê±°
            unwanted_classes = [
                'advertisement', 'banner', 'promotion', 'sponsor',
                'cookie-banner', 'popup', 'modal', 'overlay',
                'loading', 'spinner', 'progress'
            ]
            
            for class_name in unwanted_classes:
                unwanted_tags = soup.find_all(attrs={'class': re.compile(class_name, re.I)})
                for tag in unwanted_tags:
                    # í…ìŠ¤íŠ¸ê°€ ì ì€ ê²½ìš°ë§Œ ì œê±° (ë³¸ë¬¸ ë³´ì¡´)
                    if len(tag.get_text(strip=True)) < 200:
                        tag.decompose()
            
            # 5. ì™„ì „íˆ ë¹ˆ íƒœê·¸ë§Œ ì œê±°
            for tag in soup.find_all(['div', 'span', 'p']):
                text_content = tag.get_text(strip=True)
                has_images = tag.find_all(['img', 'video'])
                has_children = tag.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'span', 'div', 'a'])
                
                # í…ìŠ¤íŠ¸ê°€ ì „í˜€ ì—†ê³ , ì´ë¯¸ì§€ë„ ì—†ê³ , ìì‹ ìš”ì†Œë„ ì—†ëŠ” ê²½ìš°ë§Œ ì œê±°
                if not text_content and not has_images and len(has_children) == 0:
                    tag.decompose()
            
            # 6. ê³¼ë„í•œ ê³µë°±ë§Œ ì •ë¦¬
            for tag in soup.find_all(text=True):
                if tag.parent.name not in ['script', 'style']:
                    # ì—°ì†ëœ ê³µë°±ë§Œ ì •ë¦¬ (ì¤„ë°”ê¿ˆì€ ë³´ì¡´)
                    cleaned_text = re.sub(r'[ \t]+', ' ', tag.string)
                    # ì¤„ë°”ê¿ˆì€ ë³´ì¡´í•˜ë˜ ê³¼ë„í•œ ì¤„ë°”ê¿ˆë§Œ ì •ë¦¬
                    cleaned_text = re.sub(r'\n\s*\n\s*\n+', '\n\n', cleaned_text)
                    if cleaned_text.strip():
                        tag.replace_with(cleaned_text)
                    else:
                        tag.extract()
                        
        except Exception as e:
            logger.error(f"âŒ HTML íƒœê·¸ ì œê±° ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    def _clean_content(self, text: str) -> str:
        """ì½˜í…ì¸  ê¸°ë³¸ ì •ë¦¬ (ê³µë°± ì •ë¦¬ë§Œ ìˆ˜í–‰, LLMì—ì„œ ì´ë¯¸ ë©”íƒ€ë°ì´í„° ì œê±° ì™„ë£Œ)"""
        try:
            # ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ ì •ë¦¬ë§Œ ìˆ˜í–‰
            lines = text.split('\n')
            clean_lines = []

            for line in lines:
                line = line.strip()
                if not line or len(line) < 3:
                    continue
                clean_lines.append(line)

            final_text = '\n'.join(clean_lines)
            final_text = re.sub(r'\n\s*\n\s*\n+', '\n\n', final_text)
            final_text = final_text.strip()

            return final_text

        except Exception as e:
            logger.error(f"ì½˜í…ì¸  ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return text
    
    def setup_workflow(self):
        """LangGraph ì›Œí¬í”Œë¡œìš° ì„¤ì •"""
        
        # ìƒíƒœ ì •ì˜ - TypedDict ì‚¬ìš©
        from typing import TypedDict
        
        class AgentState(TypedDict):
            url: str
            title: Optional[str]
            raw_content: Optional[str]
            extracted_content: Optional[str]
            error: Optional[str]
        
        # ë…¸ë“œ ì •ì˜
        async def crawl_node(state: AgentState) -> AgentState:
            """ì›¹ í¬ë¡¤ë§ ë…¸ë“œ"""
            url = state["url"]

            try:
                logger.info(f"ğŸŒ [CRAWL] í¬ë¡¤ë§ ì‹œì‘: {url}")

                # í¬ë¡¤ë§ ë©”ì„œë“œ ì‚¬ìš© (ê°„ë‹¨í•˜ê²Œ await ì‚¬ìš©)
                content, title = await self._crawl_website(url)

                # ì‘ë‹µ ìƒíƒœ ë¡œê¹…
                content_length = len(content) if content else 0
                logger.info(f"ğŸ“‹ [CRAWL] í¬ë¡¤ë§ ê²°ê³¼:")
                logger.info(f"   - ì œëª©: {title if title else 'N/A'}")
                logger.info(f"   - ì½˜í…ì¸  ê¸¸ì´: {content_length}ì")

                if content and len(content) > 0:
                    preview = content[:100].replace('\n', ' ').strip()
                    logger.info(f"   - ì½˜í…ì¸  ë¯¸ë¦¬ë³´ê¸°: {preview}...")

                # ì„±ê³µ ì¡°ê±´: ì½˜í…ì¸ ê°€ ìˆê³  ì˜ë¯¸ìˆëŠ” ê¸¸ì´
                if content and len(content) > 30:
                    # _extract_contentì—ì„œ ì´ë¯¸ LLMìœ¼ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ
                    state["extracted_content"] = content
                    state["title"] = title
                    logger.info(f"âœ… [CRAWL] í¬ë¡¤ë§ ë° ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ - {content_length}ì")
                    return state

                # ì‹¤íŒ¨ ì¼€ì´ìŠ¤
                error_msg = f"í¬ë¡¤ë§ëœ ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤ (ì½˜í…ì¸  ê¸¸ì´: {content_length}ì)"

                # ì‹¤íŒ¨ ì›ì¸ ë¶„ì„
                if self._selenium_driver:
                    try:
                        current_url = self._selenium_driver.current_url
                        page_source_length = len(self._selenium_driver.page_source)

                        if current_url != url:
                            error_msg += f" - URL ë¦¬ë‹¤ì´ë ‰íŠ¸: {url} â†’ {current_url}"

                        if page_source_length == 0:
                            error_msg += " - í˜ì´ì§€ ì†ŒìŠ¤ê°€ ë¹„ì–´ìˆìŒ"

                    except Exception as e:
                        error_msg += f" - WebDriver ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}"
                else:
                    error_msg += " - WebDriverê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ"

                logger.error(f"âŒ [CRAWL] í¬ë¡¤ë§ ì‹¤íŒ¨: {error_msg}")
                state["error"] = error_msg
                return state

            except Exception as e:
                error_detail = str(e)
                logger.error(f"âŒ [CRAWL] ì˜ˆì™¸ ë°œìƒ: {error_detail}")

                # íŠ¹ì • ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„
                if "timeout" in error_detail.lower():
                    logger.error(f"   - íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜ ê°ì§€")
                elif "connection" in error_detail.lower():
                    logger.error(f"   - ì—°ê²° ì˜¤ë¥˜ ê°ì§€")
                elif "certificate" in error_detail.lower() or "ssl" in error_detail.lower():
                    logger.error(f"   - SSL/ì¸ì¦ì„œ ì˜¤ë¥˜ ê°ì§€")

                state["error"] = f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error_detail}"
                return state

        # ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ìƒì„±
        workflow = StateGraph(AgentState)

        # ë…¸ë“œ ì¶”ê°€ (crawl_nodeì—ì„œ ë³¸ë¬¸ ì¶”ì¶œê¹Œì§€ ì™„ë£Œ)
        workflow.add_node("crawl", crawl_node)

        # ì—£ì§€ ì—°ê²° (ë‹¨ìˆœí™”: crawl â†’ END)
        workflow.set_entry_point("crawl")
        workflow.add_edge("crawl", END)

        # ì»´íŒŒì¼
        self.app = workflow.compile()
    
    async def extract_content_from_url(self, url: str) -> Dict[str, Any]:
        """URLì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ"""        
        try:
            # URL ê²€ì¦
            if not self._is_valid_url(url):
                logger.error(f"âŒ [AGENT] ìœ íš¨í•˜ì§€ ì•Šì€ URL: {url}")
                return {
                    "error": "ìœ íš¨í•˜ì§€ ì•Šì€ URLì…ë‹ˆë‹¤.",
                    "url": url
                }
            
            # ì´ˆê¸° ìƒíƒœ ì„¤ì •
            initial_state = {
                "url": url,
                "title": None,
                "raw_content": None,
                "extracted_content": None,
                "error": None
            }
            
            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            result = await self.app.ainvoke(initial_state)
            
            if result.get("error"):
                logger.error(f"âŒ [AGENT] ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨ - ì˜¤ë¥˜: {result['error']}")
                return {
                    "error": result["error"],
                    "url": url
                }
            
            # ê²°ê³¼ ê²€ì¦
            extracted_content = result.get("extracted_content", "")
            title = result.get("title", "")
            
            if not extracted_content:
                logger.warning(f"âš ï¸ [AGENT] ì¶”ì¶œëœ ì½˜í…ì¸ ê°€ ë¹„ì–´ìˆìŒ")
                return {
                    "error": "ì¶”ì¶œëœ ì½˜í…ì¸ ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.",
                    "url": url
                }
            
            return {
                "url": url,
                "title": title,
                "content": extracted_content,
            }
            
        except Exception as e:
            logger.error(f"âŒ [AGENT] ì˜ˆì™¸ ë°œìƒ: {str(e)}")
            return {
                "error": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "url": url
            }
    
    def _is_valid_url(self, url: str) -> bool:
        """URL ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except:
            return False
