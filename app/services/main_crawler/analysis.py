import os
import time
from collections import Counter
from typing import Dict, Any, List, Tuple
import boto3
from datetime import datetime

from app.core.config import settings
from app.utils.logger.setup import setup_logger
from app.utils.language.generator import call_llm
from app.services.language.language_detection.detector import detect_language_with_ai
from app.prompts.language.summary import get_summary_prompt

logger = setup_logger("crawler_analysis")

# ì–¸ì–´ë³„ í°íŠ¸ ë§¤í•‘ (ìš°ì„ ìˆœìœ„ ìˆœì„œ)
LANGUAGE_FONTS = {
    'ko': ['NanumGothic', 'Nanum Gothic', 'NanumBarunGothic', 'Malgun Gothic'],
    'ja': ['Noto Sans CJK JP', 'IPAGothic', 'MS Gothic', 'Yu Gothic', 'Noto Sans JP', 'NanumGothic'],
    'zh': ['Noto Sans CJK SC', 'SimHei', 'Microsoft YaHei', 'WenQuanYi Micro Hei', 'Noto Sans SC', 'NanumGothic'],
    'en': ['DejaVu Sans', 'Arial', 'Helvetica', 'Liberation Sans']
}


class CrawlerAnalysisService:
    """í¬ë¡¤ëŸ¬ ë¶„ì„ ì„œë¹„ìŠ¤ (ìš”ì•½ + ì›Œë“œ í´ë¼ìš°ë“œ)"""

    def __init__(self):
        self.output_dir = os.path.join(settings.output_dir, "crawler_analysis")
        os.makedirs(self.output_dir, exist_ok=True)

        # NCP S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.s3_client = boto3.client(
            service_name=settings.naver_service_name,
            endpoint_url=settings.naver_endpoint_url,
            aws_access_key_id=settings.ncp_access_key,
            aws_secret_access_key=settings.ncp_secret_key
        )

    async def analyze_crawled_content(self, content: str, title: str) -> Dict[str, Any]:
        """
        í¬ë¡¤ë§ëœ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

        Args:
            content: í¬ë¡¤ë§ëœ ë³¸ë¬¸ í…ìŠ¤íŠ¸
            title: í˜ì´ì§€ ì œëª© (ì„ íƒ)

        Returns:
            Dict: {
                "summary": "ìš”ì•½ í…ìŠ¤íŠ¸",
                "ncp_url": "NCP URL"
            }
        """
        import asyncio

        try:
            total_start = time.time()
            logger.info("ğŸš€ í¬ë¡¤ë§ ì½˜í…ì¸  ë¶„ì„ ì‹œì‘")

            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
            MAX_CONTENT_LENGTH = 5000  # 5,000ìë¡œ ì œí•œ
            if len(content) > MAX_CONTENT_LENGTH:
                logger.warning(f"âš ï¸ ì½˜í…ì¸ ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({len(content)} ë¬¸ì). {MAX_CONTENT_LENGTH}ìë¡œ ì œí•œí•©ë‹ˆë‹¤.")
                content = content[:MAX_CONTENT_LENGTH]

            # 0. AI ê¸°ë°˜ ì–¸ì–´ ê°ì§€
            detection_result = await detect_language_with_ai(content)
            lang_code = detection_result.get("primary_language")
            confidence = detection_result.get("confidence", 0.0)
            logger.info(f"âœ… [AI ì–¸ì–´ê°ì§€] {lang_code}, ì‹ ë¢°ë„: {confidence:.2f}")

            # 1, 2ë¥¼ ë³‘ë ¬ ì‹¤í–‰: ìš”ì•½ ìƒì„± + ëª…ì‚¬ ì¶”ì¶œ
            step_start = time.time()
            summary_task = asyncio.create_task(self._generate_summary(content, title, lang_code))
            nouns_task = asyncio.create_task(self._extract_nouns(content, lang_code))

            summary, nouns = await asyncio.gather(summary_task, nouns_task)
            logger.info(f"âœ… [ë³‘ë ¬ì²˜ë¦¬] {time.time() - step_start:.2f}ì´ˆ | ìš”ì•½: {len(summary)} ë¬¸ì, ëª…ì‚¬: {len(nouns)} ê°œ")

            # 3. ë¶ˆìš©ì–´ ì œê±°
            filtered_nouns = self._remove_stopwords(nouns, lang_code)
            logger.info(f"âœ… [ë¶ˆìš©ì–´ì œê±°] {len(nouns)} â†’ {len(filtered_nouns)} ê°œ")

            # 4. KeyBERTë¡œ í‚¤ì›Œë“œ ìŠ¤ì½”ì–´ë§
            step_start = time.time()
            keywords_scores = await self._score_keywords_with_keybert(content, filtered_nouns, lang_code)
            logger.info(f"âœ… [í‚¤ì›Œë“œìŠ¤ì½”ì–´ë§] {time.time() - step_start:.2f}ì´ˆ | {len(keywords_scores)} ê°œ")

            # 5, 6ì„ ìˆœì°¨ ì‹¤í–‰: ê°€ì¤‘ì¹˜ ë³€í™˜ + ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±
            step_start = time.time()
            wordcloud_weights = self._convert_to_wordcloud_weights(keywords_scores)
            wordcloud_path = await self._generate_wordcloud(wordcloud_weights, lang_code)
            logger.info(f"âœ… [ì›Œë“œí´ë¼ìš°ë“œ] {time.time() - step_start:.2f}ì´ˆ | {wordcloud_path}")

            # 7. NCP TMP ë²„í‚·ì— ì—…ë¡œë“œ (ê³µê°œ ëª¨ë“œ)
            step_start = time.time()
            ncp_url = await self._upload_to_ncp_tmp(wordcloud_path)
            logger.info(f"âœ… [NCP ì—…ë¡œë“œ] {ncp_url}")

            total_time = time.time() - total_start
            logger.info(f"ğŸ‰ ì „ì²´ ë¶„ì„ ì™„ë£Œ: {total_time:.2f}ì´ˆ")

            return {
                "summary": summary,
                "ncp_url": ncp_url
            }

        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ì½˜í…ì¸  ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

    async def _generate_summary(self, content: str, title: str, lang_code: str) -> str:
        """ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        try:
            # ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
            prompt_template = get_summary_prompt(lang_code)

            # partial_variablesë¡œ page_count=""ê°€ ì„¤ì •ë˜ì–´ ìˆì–´ì„œ ì „ë‹¬í•˜ì§€ ì•Šì•„ë„ ë¨
            prompt = prompt_template.format(
                book_content=content
            )

            response = await call_llm(
                prompt=prompt,
                model=settings.default_llm_model
            )

            if hasattr(response, 'content'):
                summary = response.content
            else:
                summary = str(response)

            return summary.strip()

        except Exception as e:
            logger.error(f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")

    async def _extract_nouns(self, content: str, lang_code: str) -> List[str]:
        """ì–¸ì–´ë³„ ëª…ì‚¬ ì¶”ì¶œ + ë³µí•©ëª…ì‚¬ ê²°í•©"""
        try:
            nouns = []

            if lang_code == 'ko':
                # í•œêµ­ì–´: Kiwipiepy
                try:
                    from kiwipiepy import Kiwi

                    kiwi = Kiwi()

                    # tokenize()ë¡œ ë‹¨ì¼ ìµœì  ë¶„ì„ ê²°ê³¼ íšë“
                    tokens = kiwi.tokenize(content)

                    # ëª…ì‚¬ íƒœê·¸ë§Œ ì¶”ì¶œ (NNG: ì¼ë°˜ëª…ì‚¬, NNP: ê³ ìœ ëª…ì‚¬, NNB: ì˜ì¡´ëª…ì‚¬)
                    nouns = [token.form for token in tokens
                            if token.tag in ['NNG', 'NNP', 'NNB'] and len(token.form) > 1]

                    # ë³µí•©ëª…ì‚¬ ìƒì„± (2-gramë§Œ, ì†ë„ ìµœì í™”)
                    compound_nouns = self._extract_compound_nouns(nouns, content, n_gram_range=(2, 2))
                    nouns.extend(compound_nouns)

                    logger.info(f"âœ… Kiwipiepy ê¸°ë°˜ ëª…ì‚¬ ì¶”ì¶œ ì™„ë£Œ: {len(nouns)} ê°œ ëª…ì‚¬")

                except Exception as kiwi_error:
                    logger.warning(f"âš ï¸ Kiwipiepy ì‚¬ìš© ì‹¤íŒ¨: {str(kiwi_error)}")
                    logger.info("LLM ê¸°ë°˜ ëª…ì‚¬ ì¶”ì¶œë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                    nouns = await self._extract_nouns_with_llm(content, lang_code)

            elif lang_code == 'en':
                # ì˜ì–´: nltk pos_tag
                import nltk

                # NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ
                try:
                    nltk.data.find('tokenizers/punkt_tab')
                except LookupError:
                    try:
                        logger.info("ğŸ“¥ NLTK punkt_tab ë‹¤ìš´ë¡œë“œ ì¤‘...")
                        nltk.download('punkt_tab', quiet=True)
                        logger.info("âœ… punkt_tab ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                    except Exception as e:
                        logger.warning(f"punkt_tab ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, punktë¡œ ì¬ì‹œë„: {e}")
                        nltk.download('punkt', quiet=True)

                # NLTK 3.9+ëŠ” averaged_perceptron_tagger_eng ì‚¬ìš©
                try:
                    nltk.data.find('taggers/averaged_perceptron_tagger_eng')
                except LookupError:
                    try:
                        logger.info("ğŸ“¥ NLTK averaged_perceptron_tagger_eng ë‹¤ìš´ë¡œë“œ ì¤‘...")
                        nltk.download('averaged_perceptron_tagger_eng', quiet=True)
                        logger.info("âœ… averaged_perceptron_tagger_eng ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                    except Exception as e:
                        logger.warning(f"averaged_perceptron_tagger_eng ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, averaged_perceptron_taggerë¡œ ì¬ì‹œë„: {e}")
                        nltk.download('averaged_perceptron_tagger', quiet=True)

                tokens = nltk.word_tokenize(content)
                pos_tags = nltk.pos_tag(tokens)

                # NNìœ¼ë¡œ ì‹œì‘í•˜ëŠ” íƒœê·¸ë§Œ (ëª…ì‚¬)
                nouns = [word for word, tag in pos_tags if tag.startswith('NN')]

                # ë³µí•©ëª…ì‚¬ ìƒì„±
                compound_nouns = self._extract_compound_nouns_english(pos_tags)
                nouns.extend(compound_nouns)

                logger.info(f"âœ… nltk ê¸°ë°˜ ëª…ì‚¬ ì¶”ì¶œ ì™„ë£Œ: {len(nouns)} ê°œ")

            elif lang_code == 'ja':
                # ì¼ë³¸ì–´: fugashi
                try:
                    import fugashi

                    # fugashi Tagger ì´ˆê¸°í™”
                    tagger = fugashi.Tagger()

                    # í˜•íƒœì†Œ ë¶„ì„
                    words = tagger(content)

                    # ëª…ì‚¬ë§Œ ì¶”ì¶œ
                    for word in words:
                        if word.feature.pos1 == 'åè©' and len(word.surface) > 1:
                            nouns.append(word.surface)

                    logger.info(f"âœ… fugashi ê¸°ë°˜ ëª…ì‚¬ ì¶”ì¶œ ì™„ë£Œ: {len(nouns)} ê°œ")

                except Exception as fugashi_error:
                    logger.warning(f"âš ï¸ fugashi ì‚¬ìš© ì‹¤íŒ¨: {str(fugashi_error)}")
                    logger.info("LLM ê¸°ë°˜ ëª…ì‚¬ ì¶”ì¶œë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                    nouns = await self._extract_nouns_with_llm(content, lang_code)

            elif lang_code == 'zh':
                # ì¤‘êµ­ì–´: jieba
                import jieba.posseg as pseg

                words = pseg.cut(content)
                nouns = [word for word, tag in words if tag.startswith('n')]

                logger.info(f"âœ… jieba ê¸°ë°˜ ëª…ì‚¬ ì¶”ì¶œ ì™„ë£Œ: {len(nouns)} ê°œ")

            else:
                # ê¸°íƒ€ ì–¸ì–´: LLM ê¸°ë°˜ ì¶”ì¶œ
                logger.info(f"â„¹ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–¸ì–´({lang_code}): LLM ê¸°ë°˜ ëª…ì‚¬ ì¶”ì¶œ ì‚¬ìš©")
                nouns = await self._extract_nouns_with_llm(content, lang_code)

            return nouns

        except Exception as e:
            logger.error(f"ëª…ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            # ì‹¤íŒ¨ ì‹œ LLM ê¸°ë°˜ ì¶”ì¶œë¡œ í´ë°±
            return await self._extract_nouns_with_llm(content, lang_code)

    def _extract_compound_nouns(self, nouns: List[str], content: str, n_gram_range: Tuple[int, int] = (2, 2)) -> List[str]:
        """ë³µí•©ëª…ì‚¬ ì¶”ì¶œ (í•œêµ­ì–´ìš© - N-gram)"""
        compound_nouns = []

        try:
            # 2-gramë§Œ ìƒì„±
            freq_dict = {}

            for i in range(len(nouns) - 1):
                candidate = ' '.join(nouns[i:i+2])

                # ë³¸ë¬¸ì— ì‹¤ì œë¡œ ë“±ì¥í•˜ëŠ”ì§€ í™•ì¸
                if candidate in content:
                    freq = content.count(candidate)
                    if freq >= 2:  # 2íšŒ ì´ìƒ ë“±ì¥í•˜ëŠ” ê²ƒë§Œ
                        freq_dict[candidate] = freq

            # ë¹ˆë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 30ê°œë§Œ
            compound_nouns = sorted(freq_dict.keys(), key=lambda x: freq_dict[x], reverse=True)[:30]

            logger.info(f"ë³µí•©ëª…ì‚¬ ìƒì„±: {len(compound_nouns)} ê°œ")
            return compound_nouns

        except Exception as e:
            logger.warning(f"ë³µí•©ëª…ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return []

    def _extract_compound_nouns_english(self, pos_tags: List[Tuple[str, str]]) -> List[str]:
        """ë³µí•©ëª…ì‚¬ ì¶”ì¶œ (ì˜ì–´ìš© - ì—°ì†ëœ ëª…ì‚¬)"""
        compound_nouns = []

        try:
            current_compound = []

            for word, tag in pos_tags:
                if tag.startswith('NN'):
                    current_compound.append(word)
                else:
                    if len(current_compound) >= 2:
                        compound_nouns.append(' '.join(current_compound))
                    current_compound = []

            # ë§ˆì§€ë§‰ ë³µí•©ëª…ì‚¬ ì²˜ë¦¬
            if len(current_compound) >= 2:
                compound_nouns.append(' '.join(current_compound))

            logger.info(f"ì˜ì–´ ë³µí•©ëª…ì‚¬ ìƒì„±: {len(compound_nouns)} ê°œ")
            return compound_nouns

        except Exception as e:
            logger.warning(f"ì˜ì–´ ë³µí•©ëª…ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return []

    async def _extract_nouns_with_llm(self, content: str, lang_code: str) -> List[str]:
        """LLMì„ ì‚¬ìš©í•œ ëª…ì‚¬ ì¶”ì¶œ (í´ë°±ìš©)"""
        try:
            prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ë§Œ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ê° ëª…ì‚¬ëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸:
{content[:1500]}

ëª…ì‚¬ ëª©ë¡:"""

            response = await call_llm(
                prompt=prompt,
                model=settings.default_llm_model
            )

            if hasattr(response, 'content'):
                nouns_text = response.content
            elif isinstance(response, str):
                nouns_text = response
            else:
                nouns_text = str(response)

            # íŒŒì‹±
            nouns = [n.strip() for n in nouns_text.split(',') if n.strip()]
            return nouns

        except Exception as e:
            logger.error(f"LLM ëª…ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return []

    def _remove_stopwords(self, nouns: List[str], lang_code: str) -> List[str]:
        """ë¶ˆìš©ì–´ ì œê±° (stopwordsiso ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)"""
        try:
            from stopwordsiso import stopwords as stopwords_iso

            # ì–¸ì–´ ì½”ë“œ ë§¤í•‘ (stopwordsisoëŠ” ISO 639-1 ì½”ë“œ ì‚¬ìš©)
            lang_map = {
                'ko': 'ko',
                'en': 'en',
                'ja': 'ja',
                'zh': 'zh',
            }

            iso_lang = lang_map.get(lang_code, 'en')
            stopwords = stopwords_iso(iso_lang)

        except ImportError:
            logger.warning("stopwordsiso ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¶ˆìš©ì–´ ì œê±°ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            stopwords = set()
        except Exception as e:
            logger.warning(f"ë¶ˆìš©ì–´ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            stopwords = set()

        filtered = []
        for noun in nouns:
            # ì†Œë¬¸ì ë³€í™˜ í›„ ë¹„êµ (ê¸¸ì´ 2 ì´ìƒë§Œ ìœ ì§€)
            if noun.lower() not in stopwords and len(noun) > 1:
                filtered.append(noun)

        logger.info(f"ë¶ˆìš©ì–´ ì œê±°: {len(nouns)} â†’ {len(filtered)} ê°œ")
        return filtered

    async def _score_keywords_with_keybert(self, content: str, nouns: List[str], lang_code: str, top_n: int = 30) -> Dict[str, float]:
        """í•˜ì´ë¸Œë¦¬ë“œ í‚¤ì›Œë“œ ìŠ¤ì½”ì–´ë§ (ì˜ë¯¸ 70% + ë¹ˆë„ 30%)"""
        try:
            from keybert import KeyBERT

            # 1. ë¹ˆë„ ì ìˆ˜ ê³„ì‚° (0-1 ì •ê·œí™”)
            freq_counter = Counter(nouns)
            max_freq = max(freq_counter.values()) if freq_counter else 1
            frequency_scores = {word: count / max_freq for word, count in freq_counter.items()}

            # 2. KeyBERT ì˜ë¯¸ ì ìˆ˜ ê³„ì‚°
            kw_model = KeyBERT(model='paraphrase-multilingual-MiniLM-L12-v2')

            # ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ë¥¼ í›„ë³´ë¡œ ì œí•œ
            candidates = list(set(nouns))[:200]  # ì¤‘ë³µ ì œê±° + ìµœëŒ€ 200ê°œ

            # KeyBERTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë” ë§ì´ ì¶”ì¶œ í›„ ë‚˜ì¤‘ì— í•„í„°ë§)
            keywords = kw_model.extract_keywords(
                content,
                candidates=candidates,
                top_n=min(100, len(candidates)),  # ë” ë§ì´ ì¶”ì¶œ
                diversity=0.3
            )

            # ì˜ë¯¸ ì ìˆ˜ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            semantic_scores = {kw: score for kw, score in keywords}

            # 3. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚° (ì˜ë¯¸ 70% + ë¹ˆë„ 30%)
            hybrid_scores = {}
            for word in candidates:
                semantic_score = semantic_scores.get(word, 0.0)  # KeyBERTì— ì—†ìœ¼ë©´ 0
                frequency_score = frequency_scores.get(word, 0.0)

                # ê°€ì¤‘ í‰ê· : ì˜ë¯¸ 70%, ë¹ˆë„ 30%
                hybrid_score = (semantic_score * 0.7) + (frequency_score * 0.3)
                hybrid_scores[word] = hybrid_score

            # 4. ìƒìœ„ top_n ê°œë§Œ ì„ íƒ
            sorted_keywords = dict(sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)[:top_n])

            # 5. ìƒìœ„ í‚¤ì›Œë“œ ë¡œê¹… (ì˜ë¯¸/ë¹ˆë„ ë¶„í•´)
            logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë§ ì™„ë£Œ: {len(sorted_keywords)} ê°œ (ì˜ë¯¸ 70% + ë¹ˆë„ 30%)")

            if sorted_keywords:
                logger.info("ğŸ“Š ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ ìƒì„¸:")
                for i, (word, hybrid_score) in enumerate(list(sorted_keywords.items())[:5], 1):
                    semantic = semantic_scores.get(word, 0.0)
                    frequency = frequency_scores.get(word, 0.0)
                    freq_count = freq_counter.get(word, 0)
                    logger.info(f"  {i}. '{word}': {hybrid_score:.3f} (ì˜ë¯¸={semantic:.3f}, ë¹ˆë„={frequency:.3f}, ë“±ì¥={freq_count}íšŒ)")

            return sorted_keywords

        except ImportError:
            logger.warning("KeyBERTë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹ˆë„ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            return self._score_by_frequency(nouns, content)
        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë§ ì‹¤íŒ¨: {str(e)}")
            return self._score_by_frequency(nouns, content)

    def _score_by_frequency(self, nouns: List[str], content: str) -> Dict[str, float]:
        """ë¹ˆë„ ê¸°ë°˜ ìŠ¤ì½”ì–´ë§ (í´ë°±ìš©)"""
        freq_counter = Counter(nouns)

        # ì •ê·œí™”
        max_freq = max(freq_counter.values()) if freq_counter else 1
        normalized = {word: count / max_freq for word, count in freq_counter.items()}

        # ìƒìœ„ 30ê°œë§Œ
        sorted_keywords = dict(sorted(normalized.items(), key=lambda x: x[1], reverse=True)[:30])

        return sorted_keywords

    def _convert_to_wordcloud_weights(self, keywords_scores: Dict[str, float]) -> Dict[str, float]:
        """KeyBERT ì ìˆ˜ë¥¼ ì›Œë“œ í´ë¼ìš°ë“œìš© ê°€ì¤‘ì¹˜ë¡œ ë³€í™˜"""
        # ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ì¹˜í™˜ (ë©€í‹°ì›Œë“œ ìœ ì§€)
        converted = {}

        for keyword, score in keywords_scores.items():
            # ê³µë°± â†’ ì–¸ë”ìŠ¤ì½”ì–´
            keyword_safe = keyword.replace(' ', '_')

            # ì ìˆ˜ë¥¼ ë¹ˆë„ë¡œ ë³€í™˜ (1-100 ë²”ìœ„)
            weight = int(score * 100) + 1

            converted[keyword_safe] = weight

        logger.info(f"ì›Œë“œ í´ë¼ìš°ë“œ ê°€ì¤‘ì¹˜ ë³€í™˜ ì™„ë£Œ: {len(converted)} ê°œ")
        return converted

    async def _generate_wordcloud(self, keywords_weights: Dict[str, float], lang_code: str) -> str:
        """í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ë¡œ ì›Œë“œ í´ë¼ìš°ë“œ ì´ë¯¸ì§€ ìƒì„± (ê°œì„ ëœ ë²„ì „)"""
        try:
            from wordcloud import WordCloud
            import matplotlib.pyplot as plt
            import matplotlib
            matplotlib.use('Agg')  # ë°±ì—”ë“œ ì„¤ì •

            # ì–¸ì–´ë³„ í°íŠ¸ ì„¤ì •
            font_path = self._get_font_for_language(lang_code)

            # ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„± (ê°œì„ ëœ íŒŒë¼ë¯¸í„°)
            wordcloud = WordCloud(
                font_path=font_path,
                width=1200,
                height=600,
                background_color='white',
                colormap='viridis',
                max_words=50,  # ìµœëŒ€ ë‹¨ì–´ ìˆ˜ ì œí•œ
                min_font_size=12,  # ìµœì†Œ í°íŠ¸ í¬ê¸°
                relative_scaling=0.3,  # ìƒìœ„ì–´ í¸ì¤‘ ì™„í™” (0: ë¹ˆë„ ë¬´ì‹œ, 1: ë¹ˆë„ë§Œ ê³ ë ¤)
                normalize_plurals=False,  # ë³µìˆ˜í˜• ì •ê·œí™” ë¹„í™œì„±í™”
                collocations=False  # ì¤‘ë³µ ë°©ì§€
            ).generate_from_frequencies(keywords_weights)

            # ì´ë¯¸ì§€ ì €ì¥
            timestamp = int(time.time())
            filename = f"wordcloud_{timestamp}.png"
            output_path = os.path.join(self.output_dir, filename)

            plt.figure(figsize=(12, 6))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            plt.tight_layout(pad=0)
            plt.savefig(output_path, dpi=200, bbox_inches='tight')
            plt.close()

            return output_path

        except Exception as e:
            logger.error(f"ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")

    def _get_font_for_language(self, lang_code: str) -> str:
        """ì–¸ì–´ë³„ ì‹œìŠ¤í…œ í°íŠ¸ ì°¾ê¸° (matplotlib.font_manager ì‚¬ìš©)"""
        try:
            import matplotlib.font_manager as fm

            # ì‹œìŠ¤í…œì— ì„¤ì¹˜ëœ ëª¨ë“  í°íŠ¸ ëª©ë¡
            available_fonts = [f.name for f in fm.fontManager.ttflist]

            # ì–¸ì–´ë³„ í°íŠ¸ ìš°ì„ ìˆœìœ„
            target_fonts = LANGUAGE_FONTS.get(lang_code, LANGUAGE_FONTS['en'])

            # ìš°ì„ ìˆœìœ„ëŒ€ë¡œ í°íŠ¸ í™•ì¸
            for font_name in target_fonts:
                if font_name in available_fonts:
                    logger.info(f"ì–¸ì–´({lang_code}) í°íŠ¸ ì‚¬ìš©: {font_name}")

                    # í°íŠ¸ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
                    for font in fm.fontManager.ttflist:
                        if font.name == font_name:
                            return font.fname

            # í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
            logger.warning(f"ì–¸ì–´({lang_code})ì— ì í•©í•œ í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©")
            logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ í°íŠ¸ (ì¼ë¶€): {sorted(set(available_fonts))[:10]}")
            return None

        except Exception as e:
            logger.error(f"í°íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return None

    async def _upload_to_ncp_tmp(self, file_path: str) -> str:
        """
        ë¡œì»¬ íŒŒì¼ì„ NCP TMP ë²„í‚·ì— ì—…ë¡œë“œ (ê³µê°œ ëª¨ë“œ)

        Args:
            file_path: ì—…ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ

        Returns:
            str: ì—…ë¡œë“œëœ íŒŒì¼ì˜ NCP URL
        """
        try:
            bucket_name = settings.naver_bucket_name

            if not bucket_name:
                raise ValueError("NAVER_BUCKET_NAME í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            # íŒŒì¼ëª… ì¶”ì¶œ
            filename = os.path.basename(file_path)

            # TMP í´ë”ì— ë‚ ì§œë³„ë¡œ ì €ì¥
            date_folder = datetime.now().strftime("%Y%m%d")
            ncp_path = f"TMP/{date_folder}/{filename}"

            # NCPì— ì—…ë¡œë“œ
            with open(file_path, 'rb') as file_data:
                self.s3_client.upload_fileobj(
                    file_data,
                    bucket_name,
                    ncp_path
                )

                # íŒŒì¼ì„ ê³µê°œë¡œ ì„¤ì •
                self.s3_client.put_object_acl(
                    Bucket=bucket_name,
                    Key=ncp_path,
                    ACL='public-read'
                )

            # NCP URL ìƒì„±
            ncp_url = f"{bucket_name}/TMP/{date_folder}/{filename}"

            logger.info(f"NCP TMP ì—…ë¡œë“œ ì™„ë£Œ: {ncp_url}")

            # ë¡œì»¬ íŒŒì¼ ì‚­ì œ
            try:
                os.remove(file_path)
                logger.info(f"ë¡œì»¬ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {file_path}")
            except Exception as delete_error:
                logger.warning(f"ë¡œì»¬ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {delete_error}")

            return ncp_url

        except Exception as e:
            logger.error(f"NCP ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            raise ValueError(f"íŒŒì¼ì„ NCP ë²„í‚·ì— ì—…ë¡œë“œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
