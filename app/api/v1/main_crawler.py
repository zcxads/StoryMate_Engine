from fastapi import APIRouter, HTTPException, Depends
from datetime import datetime
import time

from app.services.main_crawler import MainCrawlerAgent, get_crawler_agent
from app.models.main_crawler.web_crawler import (
    MainCrawlRequest,
    MainCrawlResponse,
    SUPPORTED_MAIN_CRAWLER_MODELS,
    SupportedMainCrawlerModelsResponse
)
from app.config import settings
from app.utils.logger.setup import setup_logger

logger = setup_logger('main_crawler')

router = APIRouter(prefix="/main_crawler")

@router.get("/models", response_model=SupportedMainCrawlerModelsResponse)
async def get_supported_models() -> SupportedMainCrawlerModelsResponse:
    """ë©”ì¸ í¬ë¡¤ëŸ¬ì—ì„œ ì§€ì›ë˜ëŠ” AI ëª¨ë¸ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return SupportedMainCrawlerModelsResponse(
        supported_models=SUPPORTED_MAIN_CRAWLER_MODELS,
        default_model=settings.default_llm_model,
        total_count=len(SUPPORTED_MAIN_CRAWLER_MODELS)
    )

@router.post("/crawl", response_model=MainCrawlResponse)
async def extract_content(
    request: MainCrawlRequest,
    agent: MainCrawlerAgent = Depends(get_crawler_agent)
):
    """URLì—ì„œ ë³¸ë¬¸ì„ ì¦‰ì‹œ ì¶”ì¶œí•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    start_time = time.time()

    try:
        result = await agent.extract_content_from_url(str(request.url))

        # ì²˜ë¦¬ ì‹œê°„ ì¶”ê°€
        processing_time = time.time() - start_time
        result["processing_time"] = processing_time

        # ì—ëŸ¬ ë°œìƒ ì‹œ 500 ì—ëŸ¬ë¡œ í†µì¼
        if result.get("error"):
            error_msg = result['error']
            logger.error(f"âŒ [API] ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ - URL: {request.url}, ì˜¤ë¥˜: {error_msg}")
            raise HTTPException(
                status_code=500,
                detail="í¬ë¡¤ë§ì´ ë¶ˆê°€ëŠ¥í•œ ì»¨í…ì¸ ì…ë‹ˆë‹¤."
            )

        # ì„±ê³µ ë¡œê·¸
        content_length = len(result.get("content", "")) if result.get("content") else 0
        logger.info(f"ğŸ“Š [API] ê²°ê³¼ ìš”ì•½:")
        logger.info(f"   - ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
        logger.info(f"   - ì¶”ì¶œ ê¸¸ì´: {content_length}ì")
        logger.info(f"   - ì œëª©: {result.get('title', 'N/A')}")

        return MainCrawlResponse(**result)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [API] ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail="í¬ë¡¤ë§ì´ ë¶ˆê°€ëŠ¥í•œ ì»¨í…ì¸ ì…ë‹ˆë‹¤."
        )

@router.get("/health")
async def extraction_health_check():
    """
    ì„œë¹„ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    """
    return {
        "status": "healthy",
        "service": "main-crawler-agent",
        "timestamp": datetime.now().isoformat()
    }
    