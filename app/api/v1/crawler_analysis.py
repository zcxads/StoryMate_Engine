from fastapi import APIRouter, HTTPException, Depends
from datetime import datetime
import time

from app.services.main_crawler import (
    MainCrawlerAgent,
    CrawlerAnalysisService,
    get_crawler_agent,
    get_analysis_service
)
from app.models.main_crawler.analysis import (
    CrawlerAnalysisRequest,
    CrawlerAnalysisResponse
)
from app.utils.logger.setup import setup_logger

logger = setup_logger('crawler_analysis')

router = APIRouter(prefix="/crawler_analysis")


@router.post("/analyze", response_model=CrawlerAnalysisResponse)
async def analyze_crawled_content(
    request: CrawlerAnalysisRequest,
    crawler_agent: MainCrawlerAgent = Depends(get_crawler_agent),
    analysis_service: CrawlerAnalysisService = Depends(get_analysis_service)
):
    """
    URLì—ì„œ ë³¸ë¬¸ì„ í¬ë¡¤ë§í•˜ê³  ìš”ì•½ + ì›Œë“œ í´ë¼ìš°ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Returns:
        CrawlerAnalysisResponse: {
            "summary": "í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ í…ìŠ¤íŠ¸",
            "ncp_url": "ì›Œë“œ í´ë¼ìš°ë“œ ì´ë¯¸ì§€ NCP URL"
        }
    """
    start_time = time.time()

    try:
        logger.info(f"ğŸ“Š [API] í¬ë¡¤ë§ ë¶„ì„ ì‹œì‘ - URL: {request.url}")

        # 1. í¬ë¡¤ë§ ìˆ˜í–‰
        crawl_result = await crawler_agent.extract_content_from_url(str(request.url))

        if crawl_result.get("error"):
            error_msg = crawl_result['error']
            logger.error(f"âŒ [API] í¬ë¡¤ë§ ì‹¤íŒ¨ - URL: {request.url}, ì˜¤ë¥˜: {error_msg}")
            raise HTTPException(
                status_code=500,
                detail="í¬ë¡¤ë§ì´ ë¶ˆê°€ëŠ¥í•œ ì»¨í…ì¸ ì…ë‹ˆë‹¤."
            )

        content = crawl_result.get("content")
        title = crawl_result.get("title")

        if not content:
            logger.error(f"âŒ [API] í¬ë¡¤ë§ëœ ì½˜í…ì¸  ì—†ìŒ - URL: {request.url}")
            raise HTTPException(
                status_code=500,
                detail="í¬ë¡¤ë§ì´ ë¶ˆê°€ëŠ¥í•œ ì»¨í…ì¸ ì…ë‹ˆë‹¤."
            )

        logger.info(f"âœ… [API] í¬ë¡¤ë§ ì™„ë£Œ - {len(content)} ë¬¸ì")

        # 2. ë¶„ì„ ìˆ˜í–‰ (ìš”ì•½ + ì›Œë“œ í´ë¼ìš°ë“œ)
        analysis_result = await analysis_service.analyze_crawled_content(
            content=content,
            title=title
        )

        processing_time = time.time() - start_time

        logger.info(f"ğŸ“Š [API] ë¶„ì„ ì™„ë£Œ:")
        logger.info(f"   - ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
        logger.info(f"   - ìš”ì•½ ê¸¸ì´: {len(analysis_result['summary'])} ë¬¸ì")
        logger.info(f"   - ì›Œë“œ í´ë¼ìš°ë“œ: {analysis_result['ncp_url']}")

        return CrawlerAnalysisResponse(**analysis_result)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [API] ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail="í¬ë¡¤ë§ì´ ë¶ˆê°€ëŠ¥í•œ ì»¨í…ì¸ ì…ë‹ˆë‹¤."
        )


@router.get("/health")
async def analysis_health_check():
    """
    í¬ë¡¤ëŸ¬ ë¶„ì„ ì„œë¹„ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    """
    return {
        "status": "healthy",
        "service": "crawler-analysis",
        "timestamp": datetime.now().isoformat(),
        "features": [
            "ì›¹ í¬ë¡¤ë§",
            "í…ìŠ¤íŠ¸ ìš”ì•½",
            "í‚¤ì›Œë“œ ì¶”ì¶œ",
            "ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±",
            "NCP TMP ë²„í‚· ì—…ë¡œë“œ"
        ]
    }
